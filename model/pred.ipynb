{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13464f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52918d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c415c64b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "import torch\n",
    "import pandas as pd, psycopg2, os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np  \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "# --- TensorBoard ---\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from BoatRaceDataset2 import BoatRaceDataset     # ← MTL 対応版\n",
    "from DualHeadRanker import DualHeadRanker\n",
    "import itertools\n",
    "\n",
    "# --- reproducibility helpers ---\n",
    "import random  # reproducibility helpers\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Feature‑engineering registry (declarative “add / drop” infrastructure)\n",
    "# ----------------------------------------------------------------------\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Sequence, Dict\n",
    "import pandas as pd  # already imported above, but kept for clarity\n",
    "\n",
    "@dataclass\n",
    "class FeatureDef:\n",
    "    \"\"\"Declarative feature definition.\"\"\"\n",
    "    name: str\n",
    "    fn: Callable[[pd.DataFrame], pd.Series]\n",
    "    deps: Sequence[str] = field(default_factory=tuple)  # for documentation\n",
    "    dtype: str = None                            # optional cast\n",
    "\n",
    "FEATURE_REGISTRY: Dict[str, FeatureDef] = {}\n",
    "\n",
    "def register_feature(fd: FeatureDef):\n",
    "    \"\"\"Add a feature definition to the global registry.\"\"\"\n",
    "    FEATURE_REGISTRY[fd.name] = fd\n",
    "\n",
    "def apply_features(\n",
    "    df: pd.DataFrame,\n",
    "    include: Sequence[str] = None,\n",
    "    exclude: Sequence[str] = None,\n",
    "    inplace: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Materialise features declared in the registry.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Source dataframe.\n",
    "    include / exclude : list[str] | None\n",
    "        White‑/black‑lists of feature names.  `include=None` means “all”.\n",
    "    inplace : bool\n",
    "        If False (default), work on a copy to avoid side‑effects.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    names = include if include is not None else list(FEATURE_REGISTRY)\n",
    "    if exclude:\n",
    "        names = [n for n in names if n not in exclude]\n",
    "\n",
    "    for n in names:\n",
    "        fd = FEATURE_REGISTRY[n]\n",
    "        df[n] = fd.fn(df)\n",
    "        if fd.dtype:\n",
    "            df[n] = df[n].astype(fd.dtype)\n",
    "    return df\n",
    "\n",
    "# --------------------------- default features --------------------------\n",
    "def _wind_sin(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Sine of wind direction (deg → rad).\"\"\"\n",
    "    return np.sin(np.deg2rad(df[\"wind_dir_deg\"]))\n",
    "\n",
    "def _wind_cos(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Cosine of wind direction (deg → rad).\"\"\"\n",
    "    return np.cos(np.deg2rad(df[\"wind_dir_deg\"]))\n",
    "\n",
    "register_feature(FeatureDef(\"wind_sin\", _wind_sin, deps=[\"wind_dir_deg\"]))\n",
    "register_feature(FeatureDef(\"wind_cos\", _wind_cos, deps=[\"wind_dir_deg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88c820f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "with open(\"pred.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "exporter = PythonExporter()\n",
    "source, _ = exporter.from_notebook_node(nb)\n",
    "\n",
    "with open(\"pred.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bf39cf8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6927 rows from the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_9201/4121360427.py:12: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DB_CONF = {\n",
    "    \"host\":     os.getenv(\"PGHOST\", \"localhost\"),\n",
    "    \"port\":     int(os.getenv(\"PGPORT\", 5432)),\n",
    "    \"dbname\":   os.getenv(\"PGDATABASE\", \"boatrace\"),\n",
    "    \"user\":     os.getenv(\"PGUSER\", \"br_user\"),\n",
    "    \"password\": os.getenv(\"PGPASSWORD\", \"secret\"),\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONF)\n",
    "result_df = pd.read_sql(\"\"\"\n",
    "    SELECT * FROM feat.train_features3\n",
    "    WHERE race_date <= '2024-12-31'\n",
    "\"\"\", conn)\n",
    "\n",
    "print(f\"Loaded {len(result_df)} rows from the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d446358",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiichiro/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/keiichiro/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/keiichiro/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] StandardScaler will use 6 numeric cols (6 base + 0 hist)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_9201/210999142.py:29: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df[bool_cols] = result_df[bool_cols].fillna(False).astype(bool)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>venue</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wave_height</th>\n",
       "      <th>water_temp</th>\n",
       "      <th>weather_txt</th>\n",
       "      <th>wind_dir_deg</th>\n",
       "      <th>lane1_racer_id</th>\n",
       "      <th>...</th>\n",
       "      <th>lane5_first_rate</th>\n",
       "      <th>lane5_two_rate</th>\n",
       "      <th>lane5_three_rate</th>\n",
       "      <th>lane6_starts</th>\n",
       "      <th>lane6_firsts</th>\n",
       "      <th>lane6_first_rate</th>\n",
       "      <th>lane6_two_rate</th>\n",
       "      <th>lane6_three_rate</th>\n",
       "      <th>wind_sin</th>\n",
       "      <th>wind_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-06-28-12-若 松</td>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586299</td>\n",
       "      <td>0.576202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-1.252758</td>\n",
       "      <td>-2.014851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-06-29-06-若 松</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586299</td>\n",
       "      <td>0.576202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.252758</td>\n",
       "      <td>-2.014851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-06-29-07-若 松</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586299</td>\n",
       "      <td>0.576202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3475.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-1.252758</td>\n",
       "      <td>-2.014851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-06-29-08-若 松</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4445.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-1.252758</td>\n",
       "      <td>-2.014851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-06-29-09-若 松</td>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586299</td>\n",
       "      <td>0.576202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>225.0</td>\n",
       "      <td>4027.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1.252758</td>\n",
       "      <td>-2.014851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            race_key   race_date venue  air_temp  wind_speed  wave_height  \\\n",
       "0  2022-06-28-12-若 松  2022-06-28   若 松       NaN    0.586299     0.576202   \n",
       "1  2022-06-29-06-若 松  2022-06-29   若 松       NaN    0.586299     0.576202   \n",
       "2  2022-06-29-07-若 松  2022-06-29   若 松       NaN    0.586299     0.576202   \n",
       "3  2022-06-29-08-若 松  2022-06-29   若 松       NaN    0.049020     0.004539   \n",
       "4  2022-06-29-09-若 松  2022-06-29   若 松       NaN    0.586299     0.576202   \n",
       "\n",
       "   water_temp weather_txt  wind_dir_deg  lane1_racer_id  ... lane5_first_rate  \\\n",
       "0         NaN           晴         225.0          3265.0  ...         0.100000   \n",
       "1         NaN           晴         225.0          3999.0  ...         0.090909   \n",
       "2         NaN           晴         225.0          3475.0  ...         0.000000   \n",
       "3         NaN           晴         225.0          4445.0  ...         0.400000   \n",
       "4         NaN           晴         225.0          4027.0  ...         0.012821   \n",
       "\n",
       "   lane5_two_rate  lane5_three_rate  lane6_starts  lane6_firsts  \\\n",
       "0        0.800000          0.900000           4.0           0.0   \n",
       "1        0.545455          0.909091           6.0           0.0   \n",
       "2        0.142857          0.285714          10.0           0.0   \n",
       "3        0.400000          0.600000           4.0           0.0   \n",
       "4        0.179487          0.423077           6.0           0.0   \n",
       "\n",
       "   lane6_first_rate  lane6_two_rate lane6_three_rate  wind_sin  wind_cos  \n",
       "0               0.0        0.500000         0.750000 -1.252758 -2.014851  \n",
       "1               0.0        0.500000         0.500000 -1.252758 -2.014851  \n",
       "2               0.0        0.100000         0.100000 -1.252758 -2.014851  \n",
       "3               0.0        0.000000         0.250000 -1.252758 -2.014851  \n",
       "4               0.0        0.333333         0.666667 -1.252758 -2.014851  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データフレーム全体の欠損値の総数: 58221\n",
      "各列の欠損値の割合（%）:\n",
      "lane1_weight     100.0\n",
      "water_temp       100.0\n",
      "lane2_weight     100.0\n",
      "lane5_weight     100.0\n",
      "lane6_weight     100.0\n",
      "                 ...  \n",
      "race_date          0.0\n",
      "lane1_fs_flag      0.0\n",
      "lane4_fs_flag      0.0\n",
      "lane3_fs_flag      0.0\n",
      "race_key           0.0\n",
      "Length: 83, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['artifacts/wind_scaler.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 追加特徴量（Feature Registry 経由） ---\n",
    "result_df = apply_features(result_df)\n",
    "\n",
    "exclude = []\n",
    "\n",
    "for lane in range(1, 7):\n",
    "      # --- 対象列を決める（ターゲット & キー列は除外） ---\n",
    "      exclude.append(\n",
    "            f\"lane{lane}_bf_course\",\n",
    "      )\n",
    "      exclude.append(f\"lane{lane}_bf_st_time\")\n",
    "\n",
    "result_df.drop(columns=exclude, inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# numeric columns for StandardScaler\n",
    "BASE_NUM_COLS = [\"air_temp\", \"wind_speed\", \"wave_height\",\n",
    "                 \"water_temp\", \"wind_sin\", \"wind_cos\"]\n",
    "# automatically pick up newly merged rolling features (suffix *_30d)\n",
    "HIST_NUM_COLS = [c for c in result_df.columns\n",
    "                 if c.endswith(\"_30d\") and result_df[c].dtype != \"object\"]\n",
    "NUM_COLS = BASE_NUM_COLS + HIST_NUM_COLS\n",
    "print(f\"[info] StandardScaler will use {len(NUM_COLS)} numeric cols \"\n",
    "      f\"({len(BASE_NUM_COLS)} base + {len(HIST_NUM_COLS)} hist)\")\n",
    "scaler = StandardScaler().fit(result_df[NUM_COLS])\n",
    "result_df[NUM_COLS] = scaler.transform(result_df[NUM_COLS])\n",
    "\n",
    "bool_cols = [c for c in result_df.columns if c.endswith(\"_fs_flag\")]\n",
    "result_df[bool_cols] = result_df[bool_cols].fillna(False).astype(bool)\n",
    "\n",
    "# rank_cols = [f\"lane{l}_rank\" for l in range(1, 7)]\n",
    "# df[rank_cols] = df[rank_cols].fillna(7).astype(\"int32\")\n",
    "result_df.to_csv(\"artifacts/train_features.csv\", index=False)\n",
    "display(result_df.head())\n",
    "print(\"データフレーム全体の欠損値の総数:\", result_df.isnull().sum().sum())\n",
    "\n",
    "# 各列の欠損値の割合を表示（0〜1の値）\n",
    "missing_ratio = result_df.isnull().mean()\n",
    "\n",
    "# パーセント表示にする場合（見やすさのため）\n",
    "missing_ratio_percent = missing_ratio * 100\n",
    "\n",
    "print(\"各列の欠損値の割合（%）:\")\n",
    "print(missing_ratio_percent.sort_values(ascending=False))\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "scaler_filename = \"artifacts/wind_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbb3bdf5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode(col):\n",
    "    uniq = sorted(result_df[col].dropna().unique())\n",
    "    mapping = {v:i for i,v in enumerate(uniq)}\n",
    "    result_df[col + \"_id\"] = result_df[col].map(mapping).fillna(-1).astype(\"int16\")\n",
    "    return mapping\n",
    "venue2id = encode(\"venue\")\n",
    "# race_type2id = encode(\"race_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa928bde",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── sample race ──\n",
      "rank    : [np.float64(1.0), np.float64(5.0), np.float64(4.0), np.float64(2.0), np.float64(6.0), np.float64(3.0)]\n",
      "exh_time: [np.float64(6.85), np.float64(6.98), np.float64(6.96), np.float64(6.96), np.float64(6.95), np.float64(6.93)]\n",
      "st      : [np.float64(0.12), np.float64(0.09), np.float64(0.12), np.float64(0.06), np.float64(0.03), np.float64(0.13)]\n",
      "fs_flag : [np.False_, np.False_, np.False_, np.False_, np.False_, np.False_]\n",
      "weight  : [None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) ── データの“ラベル & 特徴量”を 1 行だけ覗く可視化 Snippet\n",
    "#      ★★ ここは notebook なら「1 セルだけ」実行すれば OK ★★\n",
    "# ------------------------------------------------------------\n",
    "def peek_one(df: pd.DataFrame, seed: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    ランダムに 1 レース（1 行）だけ抜き取り、順位と主要特徴量を一覧表示\n",
    "    \"\"\"\n",
    "    row = df.sample(1, random_state=seed).squeeze()\n",
    "\n",
    "    def lane_list(prefix: str):\n",
    "        return [row[f\"lane{i}_{prefix}\"] for i in range(1, 7)]\n",
    "\n",
    "    print(\"── sample race ──\")\n",
    "    print(\"rank    :\", lane_list(\"rank\"))\n",
    "    print(\"exh_time:\", lane_list(\"exh_time\"))\n",
    "    print(\"st      :\", lane_list(\"st\"))\n",
    "    print(\"fs_flag :\", lane_list(\"fs_flag\"))\n",
    "    print(\"weight  :\", lane_list(\"weight\"))\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ここで一度だけ呼んで目視確認しておくとズレにすぐ気付けます\n",
    "peek_one(result_df)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ---------------- Loss / Regularization Weights -----------------\n",
    "LAMBDA_ST = 0.1      # weight for ST‑MSE  (was 0.3)\n",
    "L1_ALPHA  = 0.02     # weight for rank‑L1 loss\n",
    "CLIP_NORM = 10.0     # gradient‑clipping threshold (was 5.0)\n",
    "RANKNET_ALPHA = 0.10   # weight for pairwise RankNet loss\n",
    "TEMPERATURE   = 0.80   # logits are divided by T at inference\n",
    "LAMBDA_WIN = 1.0        # weight for winner‑BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f0296df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl_nll should be ~0 : 2.0691652297973633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pl_nll(scores: torch.Tensor, ranks: torch.Tensor, reduce: bool = True) -> torch.Tensor:\n",
    "    scores = scores.clamp(-20.0, 20.0)        # avoid Inf/NaN\n",
    "\n",
    "    order = torch.argsort(ranks, dim=1)       # (B,6) winner→last\n",
    "    nll = torch.zeros(scores.size(0), device=scores.device)\n",
    "    s = scores.clone()\n",
    "\n",
    "    for pos in range(6):\n",
    "        log_denom = torch.logsumexp(s, dim=1)                 # (B,)\n",
    "        idx = order[:, pos]                                   # (B,)\n",
    "        chosen = s.gather(1, idx.unsqueeze(1)).squeeze(1)     # (B,)\n",
    "        nll += log_denom - chosen\n",
    "        s = s.scatter(1, idx.unsqueeze(1), float('-inf'))\n",
    "\n",
    "    return nll.mean() if reduce else nll\n",
    "\n",
    "# --- Pairwise RankNet loss ---\n",
    "def ranknet_loss(scores: torch.Tensor, ranks: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pairwise RankNet loss (cross‑entropy on all lane pairs).\n",
    "    ranks : (B,6) with 1=best … 6=worst.\n",
    "    \"\"\"\n",
    "    pair_idx = list(itertools.combinations(range(6), 2))\n",
    "    loss_acc = 0.0\n",
    "    for i, j in pair_idx:\n",
    "        S_ij = torch.sign(ranks[:, j] - ranks[:, i])  # +1 if i<j (i better)\n",
    "        diff = scores[:, i] - scores[:, j]\n",
    "        loss_acc += torch.nn.functional.softplus(-S_ij * diff).mean()\n",
    "    return loss_acc / len(pair_idx)\n",
    "\n",
    "# ── pl_nll が正しいか 3 秒で判定 ──\n",
    "scores = torch.tensor([[6, 5, 4, 3, 2, 1]], dtype=torch.float32)  # lane0 が最強\n",
    "ranks  = torch.tensor([[1, 2, 3, 4, 5, 6]], dtype=torch.int64)    # lane0 が 1 着\n",
    "print(\"pl_nll should be ~0 :\", pl_nll(scores, ranks).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d27bcdf0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m cutoff \u001b[38;5;241m=\u001b[39m latest_date \u001b[38;5;241m-\u001b[39m dt\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m)\n\u001b[1;32m      5\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiff\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# \"raw\", \"log\", \"zscore\" も試せる\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m \u001b[43mBoatRaceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrace_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m ds_val   \u001b[38;5;241m=\u001b[39m BoatRaceDataset(result_df[result_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m cutoff])\n\u001b[1;32m      9\u001b[0m loader_train \u001b[38;5;241m=\u001b[39m DataLoader(ds_train, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspace/boat_racing/model/BoatRaceDataset2.py:55\u001b[0m, in \u001b[0;36mBoatRaceDataset.__init__\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_weight:\n\u001b[1;32m     54\u001b[0m     all_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf[[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlane\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m7\u001b[39m)]]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_w\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_sd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mstd(all_w) \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstd(all_w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1e-6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ------- rank を int64 で保持 (欠損→99) -----------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3596\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3593\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3597\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/numpy/_core/_methods.py:127\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    124\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    125\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_df[\"race_date\"] = pd.to_datetime(result_df[\"race_date\"]).dt.date\n",
    "latest_date = result_df[\"race_date\"].max()\n",
    "cutoff = latest_date - dt.timedelta(days=90)\n",
    "\n",
    "mode = \"diff\"  # \"raw\", \"log\", \"zscore\" も試せる\n",
    "ds_train = BoatRaceDataset(result_df[result_df[\"race_date\"] <  cutoff])\n",
    "ds_val   = BoatRaceDataset(result_df[result_df[\"race_date\"] >= cutoff])\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "loader_val   = DataLoader(ds_val,   batch_size=512)\n",
    "\n",
    "# ------------------- ⑤ 学習ループ（LR↓ + Clip） --------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "boat_dim = ds_train.boat_dim\n",
    "model = DualHeadRanker(boat_in=boat_dim).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617e9fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using latest model: artifacts/models/model_20250818-015911.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_9201/97501340.py:60: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_recent = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            race_key   race_date  air_temp  wind_speed  wave_height  \\\n",
      "0      若松_20250421_1  2025-04-21      17.0         1.0          1.0   \n",
      "1      若松_20250421_1  2025-04-21      17.0         1.0          1.0   \n",
      "2      若松_20250421_1  2025-04-21      17.0         1.0          1.0   \n",
      "3      若松_20250421_1  2025-04-21      17.0         1.0          1.0   \n",
      "4      若松_20250421_1  2025-04-21      17.0         1.0          1.0   \n",
      "...              ...         ...       ...         ...          ...   \n",
      "54535  若松_20250717_9  2025-07-17      28.0         6.0          6.0   \n",
      "54536  若松_20250717_9  2025-07-17      28.0         6.0          6.0   \n",
      "54537  若松_20250717_9  2025-07-17      28.0         6.0          6.0   \n",
      "54538  若松_20250717_9  2025-07-17      28.0         6.0          6.0   \n",
      "54539  若松_20250717_9  2025-07-17      28.0         6.0          6.0   \n",
      "\n",
      "       water_temp weather_txt  wind_dir_deg  lane1_racer_id  lane1_weight  \\\n",
      "0            20.0           晴         337.5            4234          57.9   \n",
      "1            20.0           晴         337.5            4234          57.9   \n",
      "2            20.0           晴         337.5            4234          57.9   \n",
      "3            20.0           晴         337.5            4234          57.9   \n",
      "4            20.0           晴         337.5            4234          57.9   \n",
      "...           ...         ...           ...             ...           ...   \n",
      "54535        29.0          曇り         112.5            4906          52.0   \n",
      "54536        29.0          曇り         112.5            4906          52.0   \n",
      "54537        29.0          曇り         112.5            4906          52.0   \n",
      "54538        29.0          曇り         112.5            4906          52.0   \n",
      "54539        29.0          曇り         112.5            4906          52.0   \n",
      "\n",
      "       ...  lane5_three_rate  lane6_starts  lane6_firsts  lane6_first_rate  \\\n",
      "0      ...          0.500000           1.0           0.0               0.0   \n",
      "1      ...          0.500000           1.0           0.0               0.0   \n",
      "2      ...          0.500000           1.0           0.0               0.0   \n",
      "3      ...          0.500000           1.0           0.0               0.0   \n",
      "4      ...          0.500000           1.0           0.0               0.0   \n",
      "...    ...               ...           ...           ...               ...   \n",
      "54535  ...          0.333333           8.0           0.0               0.0   \n",
      "54536  ...          0.333333           8.0           0.0               0.0   \n",
      "54537  ...          0.333333           8.0           0.0               0.0   \n",
      "54538  ...          0.333333           8.0           0.0               0.0   \n",
      "54539  ...          0.333333           8.0           0.0               0.0   \n",
      "\n",
      "       lane6_two_rate  lane6_three_rate  first_lane  second_lane  third_lane  \\\n",
      "0               0.000             1.000           1            2           3   \n",
      "1               0.000             1.000           1            2           4   \n",
      "2               0.000             1.000           1            2           5   \n",
      "3               0.000             1.000           1            2           6   \n",
      "4               0.000             1.000           1            3           2   \n",
      "...               ...               ...         ...          ...         ...   \n",
      "54535           0.125             0.375           6            4           5   \n",
      "54536           0.125             0.375           6            5           1   \n",
      "54537           0.125             0.375           6            5           2   \n",
      "54538           0.125             0.375           6            5           3   \n",
      "54539           0.125             0.375           6            5           4   \n",
      "\n",
      "         odds  \n",
      "0        54.9  \n",
      "1        83.8  \n",
      "2       109.5  \n",
      "3       126.0  \n",
      "4        47.5  \n",
      "...       ...  \n",
      "54535  1278.0  \n",
      "54536  2122.0  \n",
      "54537  2590.0  \n",
      "54538  2445.0  \n",
      "54539  1592.0  \n",
      "\n",
      "[54540 rows x 78 columns]\n",
      "[simulate] Loaded 54540 rows (2025-01-01 – 2025-08-18).\n",
      "columns: race_key, race_date, air_temp, wind_speed, wave_height, water_temp, weather_txt, wind_dir_deg, lane1_racer_id, lane1_weight, lane1_exh_time, lane1_fs_flag, lane2_racer_id, lane2_weight, lane2_exh_time, lane2_fs_flag, lane3_racer_id, lane3_weight, lane3_exh_time, lane3_fs_flag, lane4_racer_id, lane4_weight, lane4_exh_time, lane4_fs_flag, lane5_racer_id, lane5_weight, lane5_exh_time, lane5_fs_flag, lane6_racer_id, lane6_weight, lane6_exh_time, lane6_fs_flag, lane1_starts, lane1_firsts, lane1_first_rate, lane1_two_rate, lane1_three_rate, lane2_starts, lane2_firsts, lane2_first_rate, lane2_two_rate, lane2_three_rate, lane3_starts, lane3_firsts, lane3_first_rate, lane3_two_rate, lane3_three_rate, lane4_starts, lane4_firsts, lane4_first_rate, lane4_two_rate, lane4_three_rate, lane5_starts, lane5_firsts, lane5_first_rate, lane5_two_rate, lane5_three_rate, lane6_starts, lane6_firsts, lane6_first_rate, lane6_two_rate, lane6_three_rate, first_lane, second_lane, third_lane, odds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---- Monkey‑patch ROIAnalyzer so it uses BoatRaceDataset2 (MTL) ----------\n",
    "from types import MethodType\n",
    "from BoatRaceDataset2 import BoatRaceDataset as BR2Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class _EvalDatasetMTL(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap BoatRaceDataset2 but return only 4 items (ctx, boats, lane_ids, ranks)\n",
    "    so that roi_util.py can stay unchanged.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.ds = BR2Dataset(df)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        ctx, boats, lane_ids, ranks, _, _ = self.ds[idx]\n",
    "        return ctx, boats, lane_ids, ranks\n",
    "\n",
    "def _create_loader_mtl(self, df_eval: pd.DataFrame):\n",
    "    \"\"\"Replacement for ROIAnalyzer._create_loader (MTL‑aware).\"\"\"\n",
    "    df = self.preprocess_df(df_eval, self.scaler, self.num_cols)\n",
    "    ds_eval = _EvalDatasetMTL(df)\n",
    "    loader = DataLoader(ds_eval, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    need_cols = [\"first_lane\", \"second_lane\", \"third_lane\"]\n",
    "    if all(c in df.columns for c in need_cols):\n",
    "        lanes_np = df[need_cols].to_numpy(dtype=np.int64) - 1\n",
    "    else:\n",
    "        # 予測テーブルでは真の1〜3着が無いのが普通。ダミー(0,1,2)で形だけ満たす\n",
    "        lanes_np = np.tile(np.array([0, 1, 2], dtype=np.int64), (len(df), 1))\n",
    "    return loader, df, lanes_np\n",
    "    \n",
    "\n",
    "import roi_util as _roi_util_mod\n",
    "_roi_util_mod.ROIAnalyzer._create_loader = _create_loader_mtl\n",
    "from roi_util import ROIAnalyzer\n",
    "\n",
    "\n",
    " # 最新のモデルを取得\n",
    "model_list = os.listdir(\"artifacts/models\")\n",
    "model_list = [f for f in model_list if f.endswith(\".pth\")]\n",
    "if model_list:\n",
    "    latest_model = sorted(model_list)[-1]  # 最新のモデルを選択\n",
    "    model_path = os.path.join(\"artifacts\", \"models\", latest_model)\n",
    "    print(f\"Using latest model: {model_path}\")\n",
    "    # モデルをロード\n",
    "    model = DualHeadRanker(boat_in=boat_dim)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "today = dt.date.today()\n",
    "# 2025年1月1日以降のデータを取得する場合は、以下の行を変更してください。\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "# start_date = today - dt.timedelta(days=20)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM pred.eval_with_record\n",
    "    WHERE race_date BETWEEN '{start_date}' AND '{today}'\n",
    "\"\"\"\n",
    "df_recent = pd.read_sql(query, conn)\n",
    "df_recent.to_csv(\"artifacts/eval_features.csv\", index=False)\n",
    "print(df_recent)\n",
    "\n",
    "df_recent.drop(columns=exclude, inplace=True, errors=\"ignore\")\n",
    "\n",
    "if df_recent.empty:\n",
    "    print(\"[simulate] No rows fetched for last 3 months.\")\n",
    "\n",
    "print(f\"[simulate] Loaded {len(df_recent)} rows ({start_date} – {today}).\")\n",
    "print(f\"columns: {', '.join(df_recent.columns)}\")\n",
    "\n",
    "# ---- wrap MTL model so ROIAnalyzer sees only rank scores ----\n",
    "class _RankOnly(nn.Module):\n",
    "    \"\"\"Adapter: forward() returns rank_pred tensor only, temperature-scaled.\"\"\"\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "    def forward(self, *args, **kwargs):\n",
    "        _, rank_pred, _ = self.base(*args, **kwargs)\n",
    "        return rank_pred / TEMPERATURE\n",
    "\n",
    "# ----- metrics & equity (best‑practice defaults) -----\n",
    "rank_model = _RankOnly(model).to(device)\n",
    "\n",
    "analyzer = ROIAnalyzer(model=rank_model, scaler=scaler,\n",
    "                       num_cols=NUM_COLS, device=device)\n",
    "\n",
    "# df_trifecta_met = analyzer.compute_metrics_dataframe(\n",
    "#     df_eval=df_recent,\n",
    "#     tau=5.0,                 # ← Fractional‑Kelly倍率を上げてユニットを実用域へ\n",
    "#     calibrate=\"platt\",        # ← Platt scaling で確率をキャリブレーション\n",
    "#     bet_type=\"trifecta\",  # ← 三連単を対象にする\n",
    "# )\n",
    "\n",
    "# df_trifecta_met.to_csv(\"artifacts/metrics_trifecta.csv\", index=False)\n",
    "\n",
    "# # hitが True の行だけを抽出\n",
    "# df_trifecta_met_hit = df_trifecta_met[df_trifecta_met[\"hit\"] == True]\n",
    "# df_trifecta_met_hit.to_csv(\"artifacts/metrics_trifecta_hit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08177480",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] Evaluating confidence & trifecta rank on recent predictions…\n",
      "[predict] df_eval_proc columns: race_key, race_date, air_temp, wind_speed, wave_height, water_temp, weather_txt, wind_dir_deg, lane1_racer_id, lane1_weight, lane1_exh_time, lane1_fs_flag, lane2_racer_id, lane2_weight, lane2_exh_time, lane2_fs_flag, lane3_racer_id, lane3_weight, lane3_exh_time, lane3_fs_flag, lane4_racer_id, lane4_weight, lane4_exh_time, lane4_fs_flag, lane5_racer_id, lane5_weight, lane5_exh_time, lane5_fs_flag, lane6_racer_id, lane6_weight, lane6_exh_time, lane6_fs_flag, lane1_starts, lane1_firsts, lane1_first_rate, lane1_two_rate, lane1_three_rate, lane2_starts, lane2_firsts, lane2_first_rate, lane2_two_rate, lane2_three_rate, lane3_starts, lane3_firsts, lane3_first_rate, lane3_two_rate, lane3_three_rate, lane4_starts, lane4_firsts, lane4_first_rate, lane4_two_rate, lane4_three_rate, lane5_starts, lane5_firsts, lane5_first_rate, lane5_two_rate, lane5_three_rate, lane6_starts, lane6_firsts, lane6_first_rate, lane6_two_rate, lane6_three_rate, first_lane, second_lane, third_lane, odds, wind_dir_rad, wind_sin, wind_cos, lane1_rank, lane2_rank, lane3_rank, lane4_rank, lane5_rank, lane6_rank\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiichiro/workspace/boat_racing/model/roi_util.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[bool_cols] = df[bool_cols].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 予測でも「自信度」と「正解三連単の順位」を評価し、CSV に記録 ---\n",
    "print(\"[predict] Evaluating confidence & trifecta rank on recent predictions…\")\n",
    "\n",
    "# ROIAnalyzer の前処理（スケーリング等）をそのまま使ってローダを作成\n",
    "loader_eval, _df_eval_proc, _df_odds = analyzer._create_loader(df_recent)\n",
    "# df_eval_proc のcolumnsを確認\n",
    "print(f\"[predict] df_eval_proc columns: {', '.join(_df_eval_proc.columns)}\")\n",
    "# ここで _df_eval_proc は、ROIAnalyzer._create_loader() で\n",
    "\n",
    "# 既に上で用意した rank_model は「rank_pred だけ」を返すアダプタ\n",
    "model.eval(); rank_model.eval()\n",
    "# --- prepare lists ---\n",
    "all_scores, all_ranks, all_keys, all_odds = [], [], [], []\n",
    "\n",
    "row_ptr = 0\n",
    "with torch.no_grad():\n",
    "    for ctx, boats, lane_ids, ranks in loader_eval:\n",
    "        ctx, boats, lane_ids = ctx.to(device), boats.to(device), lane_ids.to(device)\n",
    "        scores = rank_model(ctx, boats, lane_ids)\n",
    "        B = scores.size(0)\n",
    "\n",
    "        # --- core outputs ---\n",
    "        all_scores.append(scores.cpu())\n",
    "        all_ranks.append(ranks)\n",
    "\n",
    "        # --- meta values (race_key / odds) ---\n",
    "        all_keys.extend(_df_eval_proc[\"race_key\"].iloc[row_ptr : row_ptr + B].tolist())\n",
    "        row_ptr += B\n",
    "\n",
    "all_scores = torch.cat(all_scores, dim=0)   # (N,6)\n",
    "all_ranks  = torch.cat(all_ranks,  dim=0)   # (N,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc22d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiichiro/workspace/boat_racing/model/roi_util.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[bool_cols] = df[bool_cols].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[compute_metrics_dataframe] Preprocessed 54540 rows.\n",
      "n = 1\n",
      "roi : 26.69%\n",
      "n = 2\n",
      "roi : 23.39%\n",
      "n = 3\n",
      "roi : 25.28%\n",
      "n = 4\n",
      "roi : 24.68%\n",
      "n = 5\n",
      "roi : 24.74%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# all_ranksとall_scoresを結合したdfに変換\n",
    "df_scores = pd.DataFrame(all_scores.numpy(), columns=[f\"lane{i+1}_score\" for i in range(6)])\n",
    "df_ranks = pd.DataFrame(all_ranks.numpy(), columns=[f\"lane{i+1}_rank\" for i in range(6)])\n",
    "df_score_ranks = pd.concat([df_scores, df_ranks], axis=1)   \n",
    "df_score_ranks[\"race_key\"] = all_keys\n",
    "\n",
    "# df_mergedから重複行を削除\n",
    "df_score_ranks = df_score_ranks.drop_duplicates()\n",
    "\n",
    "# merge odds from df_recent by race_key\n",
    "df_score_ranks = df_score_ranks.merge(df_recent[[\"race_key\",\"odds\"]], on=\"race_key\", how=\"left\")\n",
    "\n",
    "# --- lane 列をまとめて list 化 ---\n",
    "score_cols = [f\"lane{i}_score\" for i in range(1, 7)]\n",
    "rank_cols  = [f\"lane{i}_rank\"  for i in range(1, 7)]\n",
    "\n",
    "df_score_ranks[\"scores\"] = df_score_ranks[score_cols].apply(\n",
    "    lambda r: [float(x) for x in r.values.tolist()], axis=1\n",
    ")\n",
    "df_score_ranks[\"ranks\"] = df_score_ranks[rank_cols].apply(\n",
    "    lambda r: [int(x) for x in r.values.tolist()], axis=1\n",
    ")\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "def pl_true_order_prob(scores, ranks):\n",
    "    \"\"\"\n",
    "    Plackett–Luce で '真の完全着順(1→6位)' の確率を計算。\n",
    "    scores: 長さ6のスコア配列, ranks: 長さ6の真の順位 (1=最上位)\n",
    "    \"\"\"\n",
    "    w = np.exp(np.array(scores, dtype=float))\n",
    "    # 真の順序（1→2→…→6）に並んだインデックス\n",
    "    order = [i for i, _ in sorted(enumerate(ranks), key=lambda t: t[1])]\n",
    "    denom = float(w.sum())\n",
    "    p = 1.0\n",
    "    for idx in order:\n",
    "        if denom <= 0:\n",
    "            return 0.0\n",
    "        p *= float(w[idx] / denom)\n",
    "        denom -= float(w[idx])\n",
    "    return float(p)\n",
    "\n",
    "# 6! (=720) 通りの全順位\n",
    "ALL_PERMS = list(permutations(range(6), 6))\n",
    "\n",
    "def true_order_rank(scores, ranks):\n",
    "    \"\"\"\n",
    "    全 6! 通りの PL 確率で並べたとき、真の完全順位が何番目か（1始まり）。\n",
    "    \"\"\"\n",
    "    w = np.exp(np.array(scores, dtype=float))\n",
    "    denom0 = float(w.sum())\n",
    "    true_perm = tuple(i for i, _ in sorted(enumerate(ranks), key=lambda t: t[1]))\n",
    "\n",
    "    def prob_of_perm(perm):\n",
    "        denom = denom0\n",
    "        p = 1.0\n",
    "        for idx in perm:\n",
    "            if denom <= 0:\n",
    "                return 0.0\n",
    "            p *= float(w[idx] / denom)\n",
    "            denom -= float(w[idx])\n",
    "        return p\n",
    "\n",
    "    probs = [(perm, prob_of_perm(perm)) for perm in ALL_PERMS]\n",
    "    probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for k, (perm, _) in enumerate(probs, start=1):\n",
    "        if perm == true_perm:\n",
    "            return k\n",
    "    return len(probs) + 1  # 通常は到達しない\n",
    "\n",
    "# 列の追加\n",
    "df_score_ranks[\"true_order_prob\"] = df_score_ranks.apply(\n",
    "    lambda row: pl_true_order_prob(row[\"scores\"], row[\"ranks\"]), axis=1\n",
    ")\n",
    "df_score_ranks[\"true_order_rank\"] = df_score_ranks.apply(\n",
    "    lambda row: true_order_rank(row[\"scores\"], row[\"ranks\"]), axis=1\n",
    ")\n",
    "\n",
    "# 保存\n",
    "df_score_ranks.to_csv(\"artifacts/merged_scores_ranks.csv\", index=False)\n",
    "\n",
    "# df_score_ranksを行でループ\n",
    "total_benefit = 0.0\n",
    "total_submit = 0.0\n",
    "for n in range(1, 6):\n",
    "    for index, row in df_score_ranks.iterrows():\n",
    "        total_submit += 100 * n\n",
    "        odds = row.get(\"odds\", None)\n",
    "        true_rank = row.get(\"true_order_rank\", None)\n",
    "        if true_rank <= 1 * n:\n",
    "            total_benefit += odds * 100\n",
    "\n",
    "    print(f\"n = {n}\")\n",
    "    print(f\"roi : {total_benefit / total_submit * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f277d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] 天候・風・波などの条件別分析を開始…\n",
      "[env] base rows after race_key filter: 456\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['venue'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(env_result\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_n==3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroi\u001b[39m\u001b[38;5;124m\"\u001b[39m,ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# --- call the function ---\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43mrun_env_condition_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_score_ranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_recent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# =====================================================================\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mrun_env_condition_analysis\u001b[0;34m(df_score_ranks, df_recent)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# merge env columns\u001b[39;00m\n\u001b[1;32m     23\u001b[0m env_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_key\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwind_speed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwind_dir_deg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwave_height\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mair_temp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwater_temp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvenue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m _env \u001b[38;5;241m=\u001b[39m \u001b[43mdf_recent\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdrop_duplicates(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m _base \u001b[38;5;241m=\u001b[39m _base\u001b[38;5;241m.\u001b[39mmerge(_env, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace_key\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[env] merged env cols: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[c\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m_env\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrace_key\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/core/indexes/base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['venue'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def run_env_condition_analysis(df_score_ranks, df_recent):\n",
    "    \"\"\"環境条件別のHit率/ROI集計を実行し結果CSVを出力する\"\"\"\n",
    "    print(\"[env] 天候・風・波などの条件別分析を開始…\")\n",
    "    if df_score_ranks is None or df_score_ranks.empty:\n",
    "        print(\"[env][error] df_score_ranks が空です。前段の処理を確認してください。\")\n",
    "        return\n",
    "\n",
    "    base_cols = [c for c in [\"race_key\", \"odds\", \"true_order_rank\"] if c in df_score_ranks.columns]\n",
    "    if \"race_key\" not in base_cols:\n",
    "        print(\"[env][error] 'race_key' がありません。キー列生成/マージを確認してください。\")\n",
    "        return\n",
    "\n",
    "    _base = df_score_ranks[base_cols].dropna(subset=[\"race_key\"]).copy()\n",
    "    _base[\"true_order_rank\"] = pd.to_numeric(_base[\"true_order_rank\"], errors=\"coerce\")\n",
    "    _base[\"odds\"] = pd.to_numeric(_base[\"odds\"], errors=\"coerce\")\n",
    "    print(f\"[env] base rows after race_key filter: {len(_base)}\")\n",
    "\n",
    "    if df_recent is None or df_recent.empty:\n",
    "        print(\"[env][error] df_recent が空です。クエリ取得部分をご確認ください。\")\n",
    "        return\n",
    "\n",
    "    # merge env columns\n",
    "    env_cols = [\"race_key\",\"weather_txt\",\"wind_speed\",\"wind_dir_deg\",\"wave_height\",\n",
    "                \"air_temp\",\"water_temp\",\"venue\"]\n",
    "    _env = df_recent[env_cols].drop_duplicates(\"race_key\")\n",
    "    _base = _base.merge(_env, on=\"race_key\", how=\"left\")\n",
    "    print(f\"[env] merged env cols: {[c for c in _env.columns if c!='race_key']}\")\n",
    "\n",
    "    # fill sin/cos\n",
    "    if \"wind_dir_deg\" in _base.columns:\n",
    "        _base[\"wind_sin\"] = np.sin(np.deg2rad(_base[\"wind_dir_deg\"]))\n",
    "        _base[\"wind_cos\"] = np.cos(np.deg2rad(_base[\"wind_dir_deg\"]))\n",
    "\n",
    "    # bins\n",
    "    cut = pd.cut\n",
    "    _base[\"wind_speed_bin\"] = cut(_base[\"wind_speed\"], bins=[-np.inf,2,4,6,8,np.inf])\n",
    "    _base[\"wave_height_bin\"] = cut(_base[\"wave_height\"], bins=[-np.inf,0.5,1,2,np.inf])\n",
    "    _base[\"air_temp_bin\"] = pd.qcut(_base[\"air_temp\"], 4, duplicates=\"drop\")\n",
    "    _base[\"water_temp_bin\"] = pd.qcut(_base[\"water_temp\"],4,duplicates=\"drop\")\n",
    "\n",
    "    # wind direction discrete labels\n",
    "    def deg_to_compass8(deg):\n",
    "        if pd.isna(deg): return np.nan\n",
    "        d = float(deg)%360.0\n",
    "        idx = int((d+22.5)//45)%8\n",
    "        return [\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\"][idx]\n",
    "    _base[\"wind_compass8\"] = _base[\"wind_dir_deg\"].apply(deg_to_compass8)\n",
    "    def relative_w(sv):\n",
    "        if pd.isna(sv): return np.nan\n",
    "        if sv < -0.2: return \"tailwind\"\n",
    "        if sv > 0.2:  return \"headwind\"\n",
    "        return \"cross\"\n",
    "    _base[\"wind_relative\"] = _base[\"wind_sin\"].apply(relative_w)\n",
    "\n",
    "    # summarize\n",
    "    def summarize(df, group_cols, top_n, min_n=50):\n",
    "        d = df.dropna(subset=group_cols+[\"true_order_rank\"]).copy()\n",
    "        g = d.groupby(group_cols,dropna=False)\n",
    "        out = g.apply(\n",
    "            lambda t: pd.Series({\n",
    "                \"n\":len(t),\n",
    "                \"hit_rate\":(t[\"true_order_rank\"]<=top_n).mean(),\n",
    "                \"avg_odds_on_hits\": t.loc[t[\"true_order_rank\"]<=top_n,\"odds\"].mean(),\n",
    "                \"roi\":((t.loc[t['true_order_rank']<=top_n,'odds'].sum() - len(t)*top_n)/(len(t)*top_n))\n",
    "            })\n",
    "        ).reset_index()\n",
    "        out = out[out[\"n\"]>=min_n]\n",
    "        out[\"top_n\"]=top_n\n",
    "        out[\"condition\"]= \" × \".join(group_cols)\n",
    "        return out\n",
    "\n",
    "    single_axes=[\"weather_txt\",\"wind_speed_bin\",\"wind_compass8\",\"wind_relative\",\n",
    "                 \"wave_height_bin\",\"air_temp_bin\",\"water_temp_bin\",\"venue\"]\n",
    "    pair_axes=[[\"weather_txt\",\"wind_speed_bin\"],\n",
    "               [\"weather_txt\",\"wave_height_bin\"],\n",
    "               [\"wind_relative\",\"wind_speed_bin\"],\n",
    "               [\"venue\",\"wind_relative\"]]\n",
    "\n",
    "    tables=[]\n",
    "    for N in [1,2,3,4,5]:\n",
    "        for col in single_axes:\n",
    "            if col in _base.columns:\n",
    "                tables.append(summarize(_base,[col],N))\n",
    "        for cols in pair_axes:\n",
    "            if set(cols).issubset(_base.columns):\n",
    "                tables.append(summarize(_base,cols,N))\n",
    "\n",
    "    env_result = pd.concat(tables,ignore_index=True)\n",
    "    # save & show\n",
    "    env_result.to_csv(\"artifacts/env_cond_hit_roi.csv\",index=False)\n",
    "    print(\"[env] example Top3 ROI:\")\n",
    "    print(env_result.query(\"top_n==3\").sort_values(\"roi\",ascending=False).head(10))\n",
    "\n",
    "# --- call the function ---\n",
    "run_env_condition_analysis(df_score_ranks, df_recent)\n",
    "# ====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bf67fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [race_key, race_date, air_temp, wind_speed, wave_height, water_temp, weather_txt, wind_dir_deg, lane1_racer_id, lane1_weight, lane1_exh_time, lane1_bf_st_time, lane1_bf_course, lane1_fs_flag, lane2_racer_id, lane2_weight, lane2_exh_time, lane2_bf_st_time, lane2_bf_course, lane2_fs_flag, lane3_racer_id, lane3_weight, lane3_exh_time, lane3_bf_st_time, lane3_bf_course, lane3_fs_flag, lane4_racer_id, lane4_weight, lane4_exh_time, lane4_bf_st_time, lane4_bf_course, lane4_fs_flag, lane5_racer_id, lane5_weight, lane5_exh_time, lane5_bf_st_time, lane5_bf_course, lane5_fs_flag, lane6_racer_id, lane6_weight, lane6_exh_time, lane6_bf_st_time, lane6_bf_course, lane6_fs_flag, lane1_starts, lane1_firsts, lane1_first_rate, lane1_two_rate, lane1_three_rate, lane2_starts, lane2_firsts, lane2_first_rate, lane2_two_rate, lane2_three_rate, lane3_starts, lane3_firsts, lane3_first_rate, lane3_two_rate, lane3_three_rate, lane4_starts, lane4_firsts, lane4_first_rate, lane4_two_rate, lane4_three_rate, lane5_starts, lane5_firsts, lane5_first_rate, lane5_two_rate, lane5_three_rate, lane6_starts, lane6_firsts, lane6_first_rate, lane6_two_rate, lane6_three_rate]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 74 columns]\n",
      "[predict] No rows fetched for the specified period.\n",
      "[predict] Loaded 0 rows (2025-08-09 – 2025-08-18).\n",
      "columns: race_key, race_date, air_temp, wind_speed, wave_height, water_temp, weather_txt, wind_dir_deg, lane1_racer_id, lane1_weight, lane1_exh_time, lane1_fs_flag, lane2_racer_id, lane2_weight, lane2_exh_time, lane2_fs_flag, lane3_racer_id, lane3_weight, lane3_exh_time, lane3_fs_flag, lane4_racer_id, lane4_weight, lane4_exh_time, lane4_fs_flag, lane5_racer_id, lane5_weight, lane5_exh_time, lane5_fs_flag, lane6_racer_id, lane6_weight, lane6_exh_time, lane6_fs_flag, lane1_starts, lane1_firsts, lane1_first_rate, lane1_two_rate, lane1_three_rate, lane2_starts, lane2_firsts, lane2_first_rate, lane2_two_rate, lane2_three_rate, lane3_starts, lane3_firsts, lane3_first_rate, lane3_two_rate, lane3_three_rate, lane4_starts, lane4_firsts, lane4_first_rate, lane4_two_rate, lane4_three_rate, lane5_starts, lane5_firsts, lane5_first_rate, lane5_two_rate, lane5_three_rate, lane6_starts, lane6_firsts, lane6_first_rate, lane6_two_rate, lane6_three_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_4988/4038665749.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_recent = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m predictor \u001b[38;5;241m=\u001b[39m ROIPredictor(model\u001b[38;5;241m=\u001b[39mrank_model, scaler\u001b[38;5;241m=\u001b[39mscaler,\n\u001b[1;32m     32\u001b[0m                          num_cols\u001b[38;5;241m=\u001b[39mNUM_COLS, device\u001b[38;5;241m=\u001b[39mdevice, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# (1) スコア（logits）: lane1_score..lane6_score (+ メタ列) を保存\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m pred_scores_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_recent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43minclude_meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43msave_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martifacts/pred_scores.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m display(pred_scores_df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# (2) 勝率＆フェアオッズを保存\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/boat_racing/model/roi_util.py:372\u001b[0m, in \u001b[0;36mROIPredictor.predict_scores\u001b[0;34m(self, df_eval, include_meta, save_to)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_scores\u001b[39m(\u001b[38;5;28mself\u001b[39m, df_eval: pd\u001b[38;5;241m.\u001b[39mDataFrame, include_meta: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, save_to: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    Returns a DataFrame with lane1_score..lane6_score (logits). Optionally appends meta columns.\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     loader, df, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_loader_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    374\u001b[0m     outs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/workspace/boat_racing/model/roi_util.py:366\u001b[0m, in \u001b[0;36mROIPredictor._create_loader_pred\u001b[0;34m(self, df_eval)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_loader_pred\u001b[39m(\u001b[38;5;28mself\u001b[39m, df_eval: pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DataLoader, pd\u001b[38;5;241m.\u001b[39mDataFrame, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# Use the same preprocessing & loader as ROIAnalyzer (unified path / BR2Dataset via monkey-patch)\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_eval\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 22\u001b[0m, in \u001b[0;36m_create_loader_mtl\u001b[0;34m(self, df_eval)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_loader_mtl\u001b[39m(\u001b[38;5;28mself\u001b[39m, df_eval: pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Replacement for ROIAnalyzer._create_loader (MTL‑aware).\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     ds_eval \u001b[38;5;241m=\u001b[39m _EvalDatasetMTL(df)\n\u001b[1;32m     24\u001b[0m     loader \u001b[38;5;241m=\u001b[39m DataLoader(ds_eval, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/workspace/boat_racing/model/roi_util.py:41\u001b[0m, in \u001b[0;36mROIAnalyzer.preprocess_df\u001b[0;34m(df, scaler, num_cols)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠️ wind_dir_deg が存在しないため wind_sin / wind_cos をスキップします。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m available_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m num_cols \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m---> 41\u001b[0m df[available_cols] \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mavailable_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m bool_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fs_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     44\u001b[0m df[bool_cols] \u001b[38;5;241m=\u001b[39m df[bool_cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1134\u001b[0m         )\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# prediction\n",
    "from roi_util import ROIPredictor\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "today = dt.date.today()\n",
    "# 2025年1月1日以降のデータを取得する場合は、以下の行を変更してください。\n",
    "start_date = dt.date(2025, 8, 9)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM pred.features_with_record\n",
    "    WHERE race_date BETWEEN '{start_date}' AND '{today}'\n",
    "\"\"\"\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONF)\n",
    "df_recent = pd.read_sql(query, conn)\n",
    "print(df_recent)\n",
    "df_recent.to_csv(\"artifacts/pred_features_recent.csv\", index=False)\n",
    "\n",
    "df_recent.drop(columns=exclude, inplace=True, errors=\"ignore\")\n",
    "\n",
    "if df_recent.empty:\n",
    "    print(\"[predict] No rows fetched for the specified period.\")\n",
    "\n",
    "print(f\"[predict] Loaded {len(df_recent)} rows ({start_date} – {today}).\")\n",
    "print(f\"columns: {', '.join(df_recent.columns)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# ROIPredictor でスコア＆確率を一括生成\n",
    "# ------------------------------\n",
    "predictor = ROIPredictor(model=rank_model, scaler=scaler,\n",
    "                         num_cols=NUM_COLS, device=device, batch_size=512)\n",
    "\n",
    "# (1) スコア（logits）: lane1_score..lane6_score (+ メタ列) を保存\n",
    "pred_scores_df = predictor.predict_scores(df_recent,\n",
    "                                          include_meta=True,\n",
    "                                          save_to=\"artifacts/pred_scores.csv\")\n",
    "display(pred_scores_df.head())\n",
    "\n",
    "\n",
    "# (2) 勝率＆フェアオッズを保存\n",
    "pred_probs_df = predictor.predict_win_probs(scores_df=pred_scores_df,\n",
    "                                            include_meta=True,\n",
    "                                            save_to=\"artifacts/pred_win_probs.csv\")\n",
    "display(pred_probs_df.head())\n",
    "\n",
    "# (3) 馬単/三連単の TOP‑K（PL 方式）を保存\n",
    "exa_df, tri_df = predictor.predict_exotics_topk(scores_df=pred_scores_df,\n",
    "                                                K=10,\n",
    "                                                tau=5.0,\n",
    "                                                include_meta=True,\n",
    "                                                save_exacta=\"artifacts/pred_exacta_topk.csv\",\n",
    "                                                save_trifecta=\"artifacts/pred_trifecta_topk.csv\")\n",
    "display(exa_df.head())\n",
    "display(tri_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c0e8c4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] Prediction completed and saved to artifacts directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# connのクローズ\n",
    "conn.close()\n",
    "print(\"[predict] Prediction completed and saved to artifacts directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd464791",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"scaler\": scaler_filename,\n",
    "    \"venue2id\": venue2id,\n",
    "    # \"race_type2id\": race_type2id\n",
    "}, \"cplnet_checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "boat_racing-zew2npIb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
