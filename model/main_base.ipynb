{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee6d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14df08bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "import torch\n",
    "import pandas as pd, psycopg2, os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np  \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "import datetime as dt\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "# --- TensorBoard ---\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from BoatRaceDataset_base import BoatRaceDatasetBase     # ← MTL 対応版\n",
    "from DualHeadRanker import DualHeadRanker\n",
    "import itertools\n",
    "\n",
    "# --- reproducibility helpers ---\n",
    "import random  # reproducibility helpers\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Feature‑engineering registry (declarative “add / drop” infrastructure)\n",
    "# ----------------------------------------------------------------------\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Callable, Sequence, Dict\n",
    "import pandas as pd  # already imported above, but kept for clarity\n",
    "\n",
    "@dataclass\n",
    "class FeatureDef:\n",
    "    \"\"\"Declarative feature definition.\"\"\"\n",
    "    name: str\n",
    "    fn: Callable[[pd.DataFrame], pd.Series]\n",
    "    deps: Sequence[str] = field(default_factory=tuple)  # for documentation\n",
    "    dtype: str = None                            # optional cast\n",
    "\n",
    "FEATURE_REGISTRY: Dict[str, FeatureDef] = {}\n",
    "\n",
    "def register_feature(fd: FeatureDef):\n",
    "    \"\"\"Add a feature definition to the global registry.\"\"\"\n",
    "    FEATURE_REGISTRY[fd.name] = fd\n",
    "\n",
    "def apply_features(\n",
    "    df: pd.DataFrame,\n",
    "    include: Sequence[str] = None,\n",
    "    exclude: Sequence[str] = None,\n",
    "    inplace: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    names = include if include is not None else list(FEATURE_REGISTRY)\n",
    "    if exclude:\n",
    "        names = [n for n in names if n not in exclude]\n",
    "\n",
    "    for n in names:\n",
    "        fd = FEATURE_REGISTRY[n]\n",
    "        df[n] = fd.fn(df)\n",
    "        if fd.dtype:\n",
    "            df[n] = df[n].astype(fd.dtype)\n",
    "    return df\n",
    "\n",
    "# --------------------------- default features --------------------------\n",
    "def _wind_sin(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Sine of wind direction (deg → rad).\"\"\"\n",
    "    return np.sin(np.deg2rad(df[\"wind_dir_deg\"]))\n",
    "\n",
    "def _wind_cos(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Cosine of wind direction (deg → rad).\"\"\"\n",
    "    return np.cos(np.deg2rad(df[\"wind_dir_deg\"]))\n",
    "\n",
    "\n",
    "register_feature(FeatureDef(\"wind_sin\", _wind_sin, deps=[\"wind_dir_deg\"]))\n",
    "register_feature(FeatureDef(\"wind_cos\", _wind_cos, deps=[\"wind_dir_deg\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6d4909",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nbformat\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "with open(\"main_base.ipynb\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "exporter = PythonExporter()\n",
    "source, _ = exporter.from_notebook_node(nb)\n",
    "\n",
    "with open(\"main_base.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d835c93a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6927 rows from the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2526044022.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(\"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DB_CONF = {\n",
    "    \"host\":     os.getenv(\"PGHOST\", \"localhost\"),\n",
    "    \"port\":     int(os.getenv(\"PGPORT\", 5432)),\n",
    "    \"dbname\":   os.getenv(\"PGDATABASE\", \"boatrace\"),\n",
    "    \"user\":     os.getenv(\"PGUSER\", \"br_user\"),\n",
    "    \"password\": os.getenv(\"PGPASSWORD\", \"secret\"),\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# DB 接続\n",
    "# ------------------------------------------------------------------\n",
    "conn = psycopg2.connect(**DB_CONF)\n",
    "result_df = pd.read_sql(\"\"\"\n",
    "    SELECT * FROM feat.train_features_base\n",
    "    WHERE race_date <= '2024-12-31'\n",
    "    AND venue = '若 松'\n",
    "\"\"\", conn)\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(result_df)} rows from the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8703d826",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] StandardScaler will use 6 numeric cols (6 base + 0 hist)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiichiro/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/keiichiro/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/keiichiro/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/1061446553.py:28: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df[bool_cols] = result_df[bool_cols].fillna(False).astype(bool)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_key</th>\n",
       "      <th>race_date</th>\n",
       "      <th>venue</th>\n",
       "      <th>win_pattern</th>\n",
       "      <th>air_temp</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wave_height</th>\n",
       "      <th>water_temp</th>\n",
       "      <th>weather_txt</th>\n",
       "      <th>wind_dir_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>lane5_first_rate</th>\n",
       "      <th>lane5_two_rate</th>\n",
       "      <th>lane5_three_rate</th>\n",
       "      <th>lane6_starts</th>\n",
       "      <th>lane6_firsts</th>\n",
       "      <th>lane6_first_rate</th>\n",
       "      <th>lane6_two_rate</th>\n",
       "      <th>lane6_three_rate</th>\n",
       "      <th>wind_sin</th>\n",
       "      <th>wind_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28-01-若 松</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NIGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.488259</td>\n",
       "      <td>-0.567124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>-0.304107</td>\n",
       "      <td>1.250946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28-04-若 松</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NIGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.025539</td>\n",
       "      <td>-1.138786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.304107</td>\n",
       "      <td>1.250946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28-05-若 松</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>若 松</td>\n",
       "      <td>NIGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.562818</td>\n",
       "      <td>-1.138786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28-06-若 松</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>若 松</td>\n",
       "      <td>SASHI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.562818</td>\n",
       "      <td>-1.138786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28-07-若 松</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>若 松</td>\n",
       "      <td>SASHI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.562818</td>\n",
       "      <td>-1.138786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>晴</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            race_key   race_date venue win_pattern  air_temp  wind_speed  \\\n",
       "0  2022-02-28-01-若 松  2022-02-28   若 松        NIGE       NaN   -0.488259   \n",
       "1  2022-02-28-04-若 松  2022-02-28   若 松        NIGE       NaN   -1.025539   \n",
       "2  2022-02-28-05-若 松  2022-02-28   若 松        NIGE       NaN   -1.562818   \n",
       "3  2022-02-28-06-若 松  2022-02-28   若 松       SASHI       NaN   -1.562818   \n",
       "4  2022-02-28-07-若 松  2022-02-28   若 松       SASHI       NaN   -1.562818   \n",
       "\n",
       "   wave_height  water_temp weather_txt  wind_dir_deg  ...  lane5_first_rate  \\\n",
       "0    -0.567124         NaN           晴           0.0  ...          0.000000   \n",
       "1    -1.138786         NaN           晴           0.0  ...          0.117647   \n",
       "2    -1.138786         NaN           晴           NaN  ...          0.000000   \n",
       "3    -1.138786         NaN           晴           NaN  ...          0.125000   \n",
       "4    -1.138786         NaN           晴           NaN  ...          0.000000   \n",
       "\n",
       "   lane5_two_rate  lane5_three_rate  lane6_starts  lane6_firsts  \\\n",
       "0        0.090909          0.090909          11.0           0.0   \n",
       "1        0.352941          0.470588           9.0           0.0   \n",
       "2        0.166667          0.333333           8.0           0.0   \n",
       "3        0.125000          0.375000           7.0           0.0   \n",
       "4        0.200000          0.400000           1.0           0.0   \n",
       "\n",
       "   lane6_first_rate  lane6_two_rate  lane6_three_rate  wind_sin  wind_cos  \n",
       "0               0.0        0.000000          0.181818 -0.304107  1.250946  \n",
       "1               0.0        0.222222          0.333333 -0.304107  1.250946  \n",
       "2               0.0        0.000000          0.000000       NaN       NaN  \n",
       "3               0.0        0.000000          0.000000       NaN       NaN  \n",
       "4               0.0        0.000000          0.000000       NaN       NaN  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データフレーム全体の欠損値の総数: 16659\n",
      "各列の欠損値の割合（%）:\n",
      "air_temp         100.000000\n",
      "water_temp       100.000000\n",
      "wind_cos           8.300852\n",
      "wind_sin           8.300852\n",
      "wind_dir_deg       8.300852\n",
      "                    ...    \n",
      "lane1_fs_flag      0.000000\n",
      "lane5_fs_flag      0.000000\n",
      "race_date          0.000000\n",
      "lane6_fs_flag      0.000000\n",
      "race_key           0.000000\n",
      "Length: 78, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['artifacts/wind_scaler.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_df = apply_features(result_df)\n",
    "# 重要列の drop バグ修正：bf_course / bf_st_time / weight は保持する\n",
    "# 重要列の drop バグ修正：bf_course / bf_st_time / weight は保持する\n",
    "exclude = []  # ← 重要列はドロップしない（必要があればここに追加）\n",
    "for lane in range(1, 7):\n",
    "      # --- 対象列を決める（ターゲット & キー列は除外） ---\n",
    "      exclude.append(\n",
    "            f\"lane{lane}_bf_course\",\n",
    "      )\n",
    "      exclude.append(f\"lane{lane}_bf_st_time\")\n",
    "      exclude.append(f\"lane{lane}_weight\")\n",
    "\n",
    "result_df.drop(columns=exclude, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# numeric columns for StandardScaler\n",
    "BASE_NUM_COLS = [\"air_temp\", \"wind_speed\", \"wave_height\",\n",
    "                 \"water_temp\", \"wind_sin\", \"wind_cos\"]\n",
    "# automatically pick up newly merged rolling features (suffix *_30d)\n",
    "HIST_NUM_COLS = [c for c in result_df.columns\n",
    "                 if c.endswith(\"_30d\") and result_df[c].dtype != \"object\"]\n",
    "NUM_COLS = BASE_NUM_COLS + HIST_NUM_COLS\n",
    "print(f\"[info] StandardScaler will use {len(NUM_COLS)} numeric cols \"\n",
    "      f\"({len(BASE_NUM_COLS)} base + {len(HIST_NUM_COLS)} hist)\")\n",
    "scaler = StandardScaler().fit(result_df[NUM_COLS])\n",
    "result_df[NUM_COLS] = scaler.transform(result_df[NUM_COLS])\n",
    "\n",
    "bool_cols = [c for c in result_df.columns if c.endswith(\"_fs_flag\")]\n",
    "result_df[bool_cols] = result_df[bool_cols].fillna(False).astype(bool)\n",
    "result_df.to_csv(\"artifacts/train_features_base.csv\", index=False)\n",
    "display(result_df.head())\n",
    "print(\"データフレーム全体の欠損値の総数:\", result_df.isnull().sum().sum())\n",
    "\n",
    "# 各列の欠損値の割合を表示（0〜1の値）\n",
    "missing_ratio = result_df.isnull().mean()\n",
    "\n",
    "# パーセント表示にする場合（見やすさのため）\n",
    "missing_ratio_percent = missing_ratio * 100\n",
    "\n",
    "print(\"各列の欠損値の割合（%）:\")\n",
    "print(missing_ratio_percent.sort_values(ascending=False))\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "scaler_filename = \"artifacts/wind_scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b94f6829",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encode(col):\n",
    "    uniq = sorted(result_df[col].dropna().unique())\n",
    "    mapping = {v:i for i,v in enumerate(uniq)}\n",
    "    result_df[col + \"_id\"] = result_df[col].map(mapping).fillna(-1).astype(\"int16\")\n",
    "    return mapping\n",
    "venue2id = encode(\"venue\")\n",
    "# race_type2id = encode(\"race_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b48eba6b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0) ── データの“ラベル & 特徴量”を 1 行だけ覗く可視化 Snippet\n",
    "#      ★★ ここは notebook なら「1 セルだけ」実行すれば OK ★★\n",
    "# ------------------------------------------------------------\n",
    "def peek_one(df: pd.DataFrame, seed: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    ランダムに 1 レース（1 行）だけ抜き取り、順位と主要特徴量を一覧表示\n",
    "    \"\"\"\n",
    "    row = df.sample(1, random_state=seed).squeeze()\n",
    "\n",
    "    def lane_list(prefix: str):\n",
    "        return [row[f\"lane{i}_{prefix}\"] for i in range(1, 7)]\n",
    "\n",
    "    print(\"── sample race ──\")\n",
    "    print(\"rank    :\", lane_list(\"rank\"))\n",
    "    print(\"exh_time:\", lane_list(\"exh_time\"))\n",
    "    print(\"st      :\", lane_list(\"st\"))\n",
    "    print(\"fs_flag :\", lane_list(\"fs_flag\"))\n",
    "    print(\"weight  :\", lane_list(\"weight\"))\n",
    "\n",
    "# ---------------------------------------------\n",
    "# ここで一度だけ呼んで目視確認しておくとズレにすぐ気付けます\n",
    "# peek_one(result_df)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ---------------- Loss / Regularization Weights -----------------\n",
    "LAMBDA_ST = 0.1      # weight for ST‑MSE  (was 0.3)\n",
    "L1_ALPHA  = 0.02     # weight for rank‑L1 loss\n",
    "CLIP_NORM = 10.0     # gradient‑clipping threshold (was 5.0)\n",
    "RANKNET_ALPHA = 0.10   # weight for pairwise RankNet loss\n",
    "TEMPERATURE   = 0.80   # logits are divided by T at inference\n",
    "LAMBDA_WIN = 1.0        # weight for winner‑BCE loss\n",
    "\n",
    "TOPK_K = 3\n",
    "TOPK_WEIGHTS = [3.0, 2.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8cdf762",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl_nll should be ~0 : 2.0691652297973633\n",
      "pl_nll_topk (k=3) should be ~0 : 0.4520998001098633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pl_nll(scores: torch.Tensor, ranks: torch.Tensor, reduce: bool = True) -> torch.Tensor:\n",
    "    scores = scores.clamp(-20.0, 20.0)        # avoid Inf/NaN\n",
    "\n",
    "    order = torch.argsort(ranks, dim=1)       # (B,6) winner→last\n",
    "    nll = torch.zeros(scores.size(0), device=scores.device)\n",
    "    s = scores.clone()\n",
    "\n",
    "    for pos in range(6):\n",
    "        log_denom = torch.logsumexp(s, dim=1)                 # (B,)\n",
    "        idx = order[:, pos]                                   # (B,)\n",
    "        chosen = s.gather(1, idx.unsqueeze(1)).squeeze(1)     # (B,)\n",
    "        nll += log_denom - chosen\n",
    "        s = s.scatter(1, idx.unsqueeze(1), float('-inf'))\n",
    "\n",
    "    return nll.mean() if reduce else nll\n",
    "\n",
    "def pl_nll_topk(scores: torch.Tensor,\n",
    "                ranks: torch.Tensor,\n",
    "                k: int = 3,\n",
    "                weights=None,\n",
    "                reduce: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Top-k Plackett–Luce negative log-likelihood.\n",
    "   k: number of leading positions (default 3).\n",
    "    weights: length-k list/1D Tensor of non-negative weights; if None → uniform 1.0\n",
    "    reduce: if False, returns per-sample loss (B,); otherwise mean.\n",
    "    \"\"\"\n",
    "    # numeric stability\n",
    "    scores = scores.clamp(-20.0, 20.0)\n",
    "    B, C = scores.shape\n",
    "    k = int(min(max(k, 1), C))\n",
    "\n",
    "    if weights is None:\n",
    "        w = torch.ones(k, device=scores.device, dtype=scores.dtype)\n",
    "    else:\n",
    "        w = torch.as_tensor(weights, device=scores.device, dtype=scores.dtype)\n",
    "        if w.numel() != k:\n",
    "            w = torch.ones(k, device=scores.device, dtype=scores.dtype)\n",
    "\n",
    "    order = torch.argsort(ranks, dim=1)   # (B,6) winner→\n",
    "    s = scores.clone()\n",
    "    nll = torch.zeros(B, device=scores.device, dtype=scores.dtype)\n",
    "\n",
    "    for t in range(k):\n",
    "        log_denom = torch.logsumexp(s, dim=1)             # (B,)\n",
    "        idx = order[:, t]                                  # (B,)\n",
    "        chosen = s.gather(1, idx.unsqueeze(1)).squeeze(1)  # (B,)\n",
    "        nll = nll + w[t] * (log_denom - chosen)\n",
    "        s = s.scatter(1, idx.unsqueeze(1), float('-inf'))  # mask the chosen lane\n",
    "\n",
    "    nll = nll / w.sum()\n",
    "    return nll.mean() if reduce else nll\n",
    "\n",
    "# --- Pairwise RankNet loss ---\n",
    "def ranknet_loss(scores: torch.Tensor, ranks: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pairwise RankNet loss (cross‑entropy on all lane pairs).\n",
    "    ranks : (B,6) with 1=best … 6=worst.\n",
    "    \"\"\"\n",
    "    pair_idx = list(itertools.combinations(range(6), 2))\n",
    "    loss_acc = 0.0\n",
    "    for i, j in pair_idx:\n",
    "        S_ij = torch.sign(ranks[:, j] - ranks[:, i])  # +1 if i<j (i better)\n",
    "        diff = scores[:, i] - scores[:, j]\n",
    "        loss_acc += torch.nn.functional.softplus(-S_ij * diff).mean()\n",
    "    return loss_acc / len(pair_idx)\n",
    "\n",
    "# ── pl_nll が正しいか 3 秒で判定 ──\n",
    "scores = torch.tensor([[6, 5, 4, 3, 2, 1]], dtype=torch.float32)  # lane0 が最強\n",
    "ranks  = torch.tensor([[1, 2, 3, 4, 5, 6]], dtype=torch.int64)    # lane0 が 1 着\n",
    "print(\"pl_nll should be ~0 :\", pl_nll(scores, ranks).item())\n",
    "print(\"pl_nll_topk (k=3) should be ~0 :\", pl_nll_topk(scores, ranks, k=TOPK_K, weights=TOPK_WEIGHTS).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8335ef7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boat_dim = 5\n",
      "has_motor_rates = None has_boat_rates = None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "result_df[\"race_date\"] = pd.to_datetime(result_df[\"race_date\"]).dt.date\n",
    "latest_date = result_df[\"race_date\"].max()\n",
    "cutoff = latest_date - dt.timedelta(days=90)\n",
    "\n",
    "mode = \"zscore\"  # レース内zスコア運用\n",
    "ds_train = BoatRaceDatasetBase(result_df[result_df[\"race_date\"] <  cutoff])\n",
    "ds_val   = BoatRaceDatasetBase(result_df[result_df[\"race_date\"] >= cutoff])\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "loader_val   = DataLoader(ds_val,   batch_size=512)\n",
    "\n",
    "# ------------------- ⑤ 学習ループ（LR↓ + Clip） --------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "boat_dim = ds_train.boat_dim\n",
    "print(\"boat_dim =\", ds_train.boat_dim)\n",
    "print(\"has_motor_rates =\", getattr(ds_train, \"has_motor_rates\", None),\n",
    "      \"has_boat_rates =\", getattr(ds_train, \"has_boat_rates\", None))\n",
    "model = DualHeadRanker(boat_in=boat_dim).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc6d0e1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, dataset, device):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=512)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # 6 要素を受け取り、ST は無視\n",
    "        for ctx, boats, lane_ids, ranks, *_ in loader:\n",
    "            ctx, boats = ctx.to(device), boats.to(device)\n",
    "            lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "\n",
    "            _, scores, _ = model(ctx, boats, lane_ids)   # ST & win logits are ignored\n",
    "            loss = pl_nll_topk(scores, ranks, k=TOPK_K, weights=TOPK_WEIGHTS)\n",
    "            total_loss += loss.item() * len(ctx)\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "\n",
    "# def run_experiment(data_frac, df_full, mode=\"zscore\", epochs=5, device=\"cpu\"):\n",
    "#     df_frac = df_full.sample(frac=data_frac, random_state=42)\n",
    "#     df_frac[\"race_date\"] = pd.to_datetime(df_frac[\"race_date\"]).dt.date\n",
    "#     latest_date = df_frac[\"race_date\"].max()\n",
    "#     cutoff = latest_date - dt.timedelta(days=90)  # last 3 months used as validation set\n",
    "#     ds_train = BoatRaceDataset(df_frac[df_frac[\"race_date\"] < cutoff])\n",
    "#     ds_val = BoatRaceDataset(df_frac[df_frac[\"race_date\"] >= cutoff])\n",
    "\n",
    "#     loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n",
    "#     loader_val = DataLoader(ds_val, batch_size=512)\n",
    "\n",
    "#     boat_dim = ds_train.boat_dim\n",
    "#     model = DualHeadRanker(boat_in=boat_dim).to(device)\n",
    "#     opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-5)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for ctx, boats, lane_ids, ranks, *_ in loader_train:\n",
    "#             ctx, boats = ctx.to(device), boats.to(device)\n",
    "#             lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "\n",
    "#             _, scores, _ = model(ctx, boats, lane_ids)        # discard ST & win head\n",
    "#             loss = (pl_nll(scores, ranks, reduce=False) *\n",
    "#                     torch.where(ranks[:,0]==1,\n",
    "#                                 torch.tensor(1.0, device=ranks.device),\n",
    "#                                 torch.tensor(1.5, device=ranks.device))).mean()\n",
    "#             opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "#     train_loss = evaluate_model(model, ds_train, device)\n",
    "#     val_loss = evaluate_model(model, ds_val, device)\n",
    "#     return train_loss, val_loss\n",
    "\n",
    "# # 学習曲線の描画\n",
    "# def plot_learning_curve(df, device):\n",
    "#     data_fracs = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "#     results = []\n",
    "\n",
    "#     for frac in data_fracs:\n",
    "#         tr_loss, val_loss = run_experiment(frac, df, device=device)\n",
    "#         print(f\"Data frac {frac:.2f} → Train: {tr_loss:.4f} / Val: {val_loss:.4f}\")\n",
    "#         results.append((frac, tr_loss, val_loss))\n",
    "\n",
    "#     fracs, tr_losses, val_losses = zip(*results)\n",
    "#     plt.plot(fracs, tr_losses, label=\"Train Loss\")\n",
    "#     plt.plot(fracs, val_losses, label=\"Val Loss\")\n",
    "#     plt.xlabel(\"Training Data Fraction\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.title(\"Learning Curve\")\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.show()\n",
    "\n",
    "# def overfit_tiny(df: pd.DataFrame, device: str = \"cpu\"):\n",
    "#     \"\"\"\n",
    "#     データセットを 10 行だけに縮小し、500 step で過学習できるか検証\n",
    "#     \"\"\"\n",
    "#     tiny_df = df.sample(10, random_state=1).reset_index(drop=True)\n",
    "#     tiny_ds = BoatRaceDataset(tiny_df)\n",
    "#     tiny_loader = DataLoader(tiny_ds, batch_size=10, shuffle=True)\n",
    "\n",
    "#     net = DualHeadRanker().to(device)\n",
    "#     opt = torch.optim.AdamW(net.parameters(), lr=3e-3)\n",
    "\n",
    "#     for _ in range(500):\n",
    "#         ctx, boats, lane_ids, ranks, st_true, st_mask = next(iter(tiny_loader))\n",
    "#         ctx, boats = ctx.to(device), boats.to(device)\n",
    "#         lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "#         st_true, st_mask = st_true.to(device), st_mask.to(device)\n",
    "#         st_pred, scores, _ = net(ctx, boats, lane_ids)\n",
    "#         pl_loss = (pl_nll(scores, ranks, reduce=False) *\n",
    "#                    torch.where(ranks[:,0]==1,\n",
    "#                                torch.tensor(1.0, device=ranks.device),\n",
    "#                                torch.tensor(1.5, device=ranks.device))).mean()\n",
    "#         mse_st = ((st_pred - st_true) ** 2 * st_mask.float()).sum() / st_mask.float().sum()\n",
    "#         loss = pl_loss + LAMBDA_ST * mse_st\n",
    "#         opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "#     print(\"[tiny] final loss:\", loss.item())\n",
    "\n",
    "# RUN_DIAG = True\n",
    "\n",
    "# if RUN_DIAG:\n",
    "#     print(\"[diag] Running learning‑curve vs. data fraction …\")\n",
    "#     plot_learning_curve(df, device)\n",
    "#     print(\"[diag] Running 10‑row overfit_tiny() …\")\n",
    "#     overfit_tiny(df, device)\n",
    "#     print(\"[diag]   ► finished quick diagnostics\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2533c0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  train_nll 2.4514  val_nll 1.2694  |grad| 2.47e-01\n",
      "ST MSE: 0.2134  ST MAE: 0.4263\n",
      "epoch  1  train_nll 2.0109  val_nll 1.4847  |grad| 1.91e-01\n",
      "ST MSE: 0.2877  ST MAE: 0.4848\n",
      "epoch  2  train_nll 1.9974  val_nll 1.3058  |grad| 1.17e-01\n",
      "ST MSE: 0.2288  ST MAE: 0.4179\n",
      "epoch  3  train_nll 1.9871  val_nll 1.3178  |grad| 8.51e-02\n",
      "ST MSE: 0.1816  ST MAE: 0.3716\n",
      "epoch  4  train_nll 2.0062  val_nll 1.3907  |grad| 9.27e-02\n",
      "ST MSE: 0.3316  ST MAE: 0.5076\n",
      "epoch  5  train_nll 1.9733  val_nll 1.2347  |grad| 7.68e-02\n",
      "ST MSE: 0.1013  ST MAE: 0.2809\n",
      "epoch  6  train_nll 1.9033  val_nll 1.3893  |grad| 9.27e-02\n",
      "ST MSE: 0.1458  ST MAE: 0.3054\n",
      "epoch  7  train_nll 1.9591  val_nll 1.3087  |grad| 8.90e-02\n",
      "ST MSE: 0.1189  ST MAE: 0.2783\n",
      "epoch  8  train_nll 1.8965  val_nll 1.2443  |grad| 1.03e-01\n",
      "ST MSE: 0.0683  ST MAE: 0.2198\n",
      "epoch  9  train_nll 1.9568  val_nll 1.2068  |grad| 1.07e-01\n",
      "ST MSE: 0.0753  ST MAE: 0.2176\n",
      "epoch 10  train_nll 1.8604  val_nll 1.3055  |grad| 8.78e-02\n",
      "ST MSE: 0.0193  ST MAE: 0.1041\n",
      "epoch 11  train_nll 1.9115  val_nll 1.3329  |grad| 1.09e-01\n",
      "ST MSE: 0.0635  ST MAE: 0.2055\n",
      "epoch 12  train_nll 1.9102  val_nll 1.2633  |grad| 1.14e-01\n",
      "ST MSE: 0.0776  ST MAE: 0.2116\n",
      "epoch 13  train_nll 1.9019  val_nll 1.2026  |grad| 1.31e-01\n",
      "ST MSE: 0.0252  ST MAE: 0.1190\n",
      "epoch 14  train_nll 1.8778  val_nll 1.2672  |grad| 1.41e-01\n",
      "ST MSE: 0.0182  ST MAE: 0.1008\n",
      "epoch 15  train_nll 1.8911  val_nll 1.2410  |grad| 1.38e-01\n",
      "ST MSE: 0.0337  ST MAE: 0.1340\n",
      "epoch 16  train_nll 1.9249  val_nll 1.2186  |grad| 1.77e-01\n",
      "ST MSE: 0.0199  ST MAE: 0.1035\n",
      "epoch 17  train_nll 1.8726  val_nll 1.2020  |grad| 1.87e-01\n",
      "ST MSE: 0.0124  ST MAE: 0.0859\n",
      "epoch 18  train_nll 1.8371  val_nll 1.2885  |grad| 8.94e-02\n",
      "ST MSE: 0.0091  ST MAE: 0.0745\n",
      "epoch 19  train_nll 1.8795  val_nll 1.2508  |grad| 7.47e-02\n",
      "ST MSE: 0.0129  ST MAE: 0.0906\n",
      "[export] Saving raw softmax probabilities for calibration …\n",
      "Model saved to artifacts/models/model_20250830-025942.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# print(f\"train: {len(ds_train)}  val: {len(ds_val)}\"\n",
    "\n",
    "EPOCHS = 20\n",
    "# --- TensorBoard setup ---\n",
    "log_dir = os.path.join(\"artifacts\", \"tb\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch == 0:                  # 1 エポック目だけ試す例\n",
    "        ctx, boats, lane_ids, ranks, st_true, st_mask = next(iter(loader_train))\n",
    "        ctx, boats = ctx.to(device), boats.to(device)\n",
    "        lane_ids = lane_ids.to(device)\n",
    "        st_true, st_mask = st_true.to(device), st_mask.to(device)\n",
    "\n",
    "        st_pred, scores, win_logits = model(ctx, boats, lane_ids)\n",
    "        (st_pred.sum() + scores.sum() + win_logits.sum()).backward()     # ダミー backward\n",
    "        grad_norm = sum(p.grad.abs().mean().item() for p in model.parameters())\n",
    "        # print(f\"[debug] average |grad| = {grad_norm:.3e}\")\n",
    "    # ---- train ----\n",
    "    model.train(); tr_sum = 0\n",
    "    grad_sum, grad_steps = 0.0, 0\n",
    "    for ctx, boats, lane_ids, ranks, st_true, st_mask in loader_train:\n",
    "        ctx, boats = ctx.to(device), boats.to(device)\n",
    "        lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "        st_true, st_mask = st_true.to(device), st_mask.to(device)\n",
    "\n",
    "        st_pred, scores, win_logits = model(ctx, boats, lane_ids)\n",
    "        loss_each = pl_nll_topk(scores, ranks, k=TOPK_K, weights=TOPK_WEIGHTS, reduce=False)  # (B,)\n",
    "        sample_weight = torch.where(ranks[:, 0] == 1,               # lane1 winner?\n",
    "                                    torch.tensor(1.0, device=ranks.device),\n",
    "                                    torch.tensor(1.5, device=ranks.device))\n",
    "        pl_loss = (loss_each * sample_weight).mean()\n",
    "        mse_st = ((st_pred - st_true) ** 2 * st_mask.float()).sum() / st_mask.float().sum()\n",
    "        pred_rank = scores.argsort(dim=1, descending=True).argsort(dim=1) + 1  # 1〜6 着になるよう変換\n",
    "        l1_loss = nn.L1Loss()(pred_rank.float(), ranks.float())\n",
    "        rn_loss = ranknet_loss(scores, ranks)\n",
    "        winner_true = (ranks == 1).float()            # one‑hot (B,6)\n",
    "        bce_win = nn.BCEWithLogitsLoss()(win_logits, winner_true)\n",
    "        loss = pl_loss + LAMBDA_ST * mse_st + L1_ALPHA * l1_loss + \\\n",
    "               RANKNET_ALPHA * rn_loss + LAMBDA_WIN * bce_win\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)  # ★勾配爆発対策★\n",
    "        opt.step()\n",
    "\n",
    "        # ---- gradient magnitude tracking ----\n",
    "        g_tot, g_cnt = 0.0, 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                g_tot += p.grad.detach().abs().mean().item()\n",
    "                g_cnt += 1\n",
    "        grad_sum += g_tot / max(g_cnt, 1)\n",
    "        grad_steps += 1\n",
    "\n",
    "        tr_sum += loss.item() * len(ctx)\n",
    "\n",
    "    tr_nll = tr_sum / len(loader_train.dataset)\n",
    "\n",
    "    # ---- validation ----\n",
    "    model.eval(); val_sum = 0\n",
    "    # --- validation: also compute st MSE/MAE\n",
    "    mse_st_accum, mae_st_accum, n_st = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for ctx, boats, lane_ids, ranks, st_true, st_mask in loader_val:\n",
    "            ctx, boats = ctx.to(device), boats.to(device)\n",
    "            lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "            st_true, st_mask = st_true.to(device), st_mask.to(device)\n",
    "            st_pred, scores, _ = model(ctx, boats, lane_ids)\n",
    "            pl_loss = pl_nll_topk(scores, ranks, k=TOPK_K, weights=TOPK_WEIGHTS)\n",
    "            # ST MSE\n",
    "            mse_st = ((st_pred - st_true) ** 2 * st_mask.float()).sum() / st_mask.float().sum()\n",
    "            # ST MAE\n",
    "            abs_err = (st_pred - st_true).abs() * st_mask.float()\n",
    "            mae_st = abs_err.sum() / st_mask.float().sum()\n",
    "            # accumulate for epoch\n",
    "            mse_st_accum += ( ((st_pred - st_true) ** 2) * st_mask.float() ).sum().item()\n",
    "            mae_st_accum += abs_err.sum().item()\n",
    "            n_st += st_mask.float().sum().item()\n",
    "            pred_rank = scores.argsort(dim=1, descending=True).argsort(dim=1) + 1\n",
    "            l1_loss = nn.L1Loss()(pred_rank.float(), ranks.float())\n",
    "            rn_loss = ranknet_loss(scores, ranks)\n",
    "            total = pl_loss + LAMBDA_ST * mse_st + L1_ALPHA * l1_loss + RANKNET_ALPHA * rn_loss\n",
    "            val_sum += total.item() * len(ctx)\n",
    "\n",
    "    val_nll = val_sum / len(loader_val.dataset)\n",
    "    # epoch-wise st metrics\n",
    "    st_mse_val = mse_st_accum / n_st if n_st > 0 else float('nan')\n",
    "    st_mae_val = mae_st_accum / n_st if n_st > 0 else float('nan')\n",
    "\n",
    "    avg_grad = grad_sum / max(grad_steps, 1)\n",
    "    writer.add_scalar(\"diag/avg_grad\", avg_grad, epoch)\n",
    "    print(f\"epoch {epoch:2d}  train_nll {tr_nll:.4f}  val_nll {val_nll:.4f}  |grad| {avg_grad:.2e}\")\n",
    "    print(f\"ST MSE: {st_mse_val:.4f}  ST MAE: {st_mae_val:.4f}\")\n",
    "\n",
    "    # ---- accuracy & 三連単的中率 ----\n",
    "    def top1_accuracy(scores, ranks):\n",
    "        pred_top1 = scores.argmax(dim=1)\n",
    "        true_top1 = (ranks == 1).nonzero(as_tuple=True)[1]\n",
    "        return (pred_top1 == true_top1).float().mean().item()\n",
    "\n",
    "    def trifecta_hit_rate(scores, ranks):\n",
    "        \"\"\"\n",
    "        三連単的中率（予測スコア上位3頭の順番が、実際の1〜3着と完全一致する割合）\n",
    "        \"\"\"\n",
    "        pred_top3 = torch.topk(scores, k=3, dim=1).indices\n",
    "        true_top3 = torch.topk(-ranks, k=3, dim=1).indices  # 小さい順に1〜3着\n",
    "        hit = [p.tolist() == t.tolist() for p, t in zip(pred_top3, true_top3)]\n",
    "        return sum(hit) / len(hit)\n",
    "\n",
    "    # accuracy 評価\n",
    "    model.eval(); all_scores, all_ranks = [], []\n",
    "    with torch.no_grad():\n",
    "        for ctx, boats, lane_ids, ranks, _, _ in loader_val:\n",
    "            ctx, boats = ctx.to(device), boats.to(device)\n",
    "            lane_ids = lane_ids.to(device)\n",
    "            _, scores, _ = model(ctx, boats, lane_ids)\n",
    "            all_scores.append(scores.cpu())\n",
    "            all_ranks.append(ranks)\n",
    "\n",
    "    all_scores = torch.cat(all_scores, dim=0)\n",
    "    all_ranks = torch.cat(all_ranks, dim=0)\n",
    "\n",
    "    acc_top1 = top1_accuracy(all_scores, all_ranks)\n",
    "    acc_tri3 = trifecta_hit_rate(all_scores, all_ranks)\n",
    "    writer.add_scalar(\"loss/train_nll\",  tr_nll,  epoch)\n",
    "    writer.add_scalar(\"loss/val_nll\",    val_nll, epoch)\n",
    "    writer.add_scalar(\"acc/top1\",        acc_top1, epoch)\n",
    "    writer.add_scalar(\"acc/trifecta_hit\", acc_tri3, epoch)\n",
    "    # --- ST metrics to TensorBoard\n",
    "    writer.add_scalar(\"st/mse\", st_mse_val, epoch)\n",
    "    writer.add_scalar(\"st/mae\", st_mae_val, epoch)\n",
    "\n",
    "    # --- スコア分散と三連単ランク分析 ---\n",
    "    def score_confidence_analysis(scores: torch.Tensor) -> torch.Tensor:\n",
    "        return scores.var(dim=1)\n",
    "\n",
    "    def get_trifecta_rank(scores: torch.Tensor, true_ranks: torch.Tensor) -> list:\n",
    "        from itertools import permutations\n",
    "        B = scores.size(0)\n",
    "        results = []\n",
    "\n",
    "        for b in range(B):\n",
    "            score = scores[b]\n",
    "            true_rank = true_ranks[b]\n",
    "            true_top3 = [i for i, r in enumerate(true_rank.tolist()) if r <= 3]\n",
    "            true_top3_sorted = [x for _, x in sorted(zip(true_rank[true_top3], true_top3))]\n",
    "\n",
    "            trifecta_list = list(permutations(range(6), 3))\n",
    "            trifecta_scores = [(triplet, score[list(triplet)].sum().item()) for triplet in trifecta_list]\n",
    "            trifecta_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            for rank_idx, (triplet, _) in enumerate(trifecta_scores):\n",
    "                if list(triplet) == true_top3_sorted:\n",
    "                    results.append(rank_idx + 1)\n",
    "                    break\n",
    "            else:\n",
    "                results.append(121)\n",
    "        return results\n",
    "\n",
    "    score_vars = score_confidence_analysis(all_scores)\n",
    "    tri_ranks = get_trifecta_rank(all_scores, all_ranks)\n",
    "    mean_var = score_vars.mean().item()\n",
    "    median_var = score_vars.median().item()\n",
    "    mean_tri_rank = np.mean(tri_ranks)\n",
    "\n",
    "    # print(f\"スコア分散の平均: {mean_var:.4f}\")\n",
    "    # print(f\"スコア分散の中央値: {median_var:.4f}\")\n",
    "    # print(f\"正解三連単の予測ランク（平均）: {mean_tri_rank:.2f}\")\n",
    "\n",
    "    writer.add_scalar(\"score_variance/mean\", mean_var, epoch)\n",
    "    writer.add_scalar(\"score_variance/median\", median_var, epoch)\n",
    "    writer.add_scalar(\"trifecta_rank/mean\", mean_tri_rank, epoch)\n",
    "\n",
    "    # print(f\"Top-1 Acc: {acc_top1:.3f}   Trifecta Hit: {acc_tri3:.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    #  Export raw softmax probabilities (6‑class) + winner label (val set)\n",
    "    # ------------------------------------------------------------------\n",
    "    if epoch == EPOCHS - 1:   # export once after final epoch\n",
    "        print(\"[export] Saving raw softmax probabilities for calibration …\")\n",
    "        with torch.no_grad():\n",
    "            ctx_all, boats_all, lane_ids_all, ranks_all = [], [], [], []\n",
    "            for ctx, boats, lane_ids, ranks, *_ in loader_val:\n",
    "                ctx_all.append(ctx); boats_all.append(boats)\n",
    "                lane_ids_all.append(lane_ids); ranks_all.append(ranks)\n",
    "\n",
    "            ctx_all   = torch.cat(ctx_all).to(device)\n",
    "            boats_all = torch.cat(boats_all).to(device)\n",
    "            lane_ids_all = torch.cat(lane_ids_all).to(device)\n",
    "            ranks_all = torch.cat(ranks_all).to(device)\n",
    "\n",
    "            _, score_all, _ = model(ctx_all, boats_all, lane_ids_all)\n",
    "            prob_all = torch.softmax(score_all / TEMPERATURE, dim=1)  # (N,6)\n",
    "\n",
    "            # winner label: 0..5\n",
    "            winner_idx = (ranks_all == 1).nonzero(as_tuple=True)[1].cpu().numpy()\n",
    "\n",
    "            df_probs = pd.DataFrame(prob_all.cpu().numpy(),\n",
    "                                    columns=[f\"prob_lane{i}\" for i in range(1,7)])\n",
    "            df_probs[\"winner\"] = winner_idx\n",
    "            probs_path = \"artifacts/raw_probs_val.csv\"\n",
    "\n",
    "    # ---- 学習ログを CSV へ追記保存 ----\n",
    "    import csv\n",
    "    os.makedirs(\"artifacts\", exist_ok=True)\n",
    "    log_path = f\"artifacts/train_{mode}.csv\"\n",
    "    # 1回目だけヘッダーを書き込む\n",
    "    write_header = epoch == 0 and not os.path.exists(log_path)\n",
    "    with open(log_path, mode=\"a\", newline=\"\") as f:\n",
    "        writer_csv = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer_csv.writerow([\"epoch\", \"train_nll\", \"val_nll\", \"top1_acc\", \"trifecta_hit\",\n",
    "                                 \"score_var_mean\", \"score_var_median\", \"tri_rank_mean\",\n",
    "                                 \"st_mse\", \"st_mae\"])\n",
    "        writer_csv.writerow([epoch, tr_nll, val_nll, acc_top1, acc_tri3,\n",
    "                             mean_var, median_var, mean_tri_rank,\n",
    "                             st_mse_val, st_mae_val])\n",
    "\n",
    "\n",
    "# --- Close TensorBoard writer after training ---\n",
    "writer.close()\n",
    "\n",
    "# modelの保存\n",
    "now = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "model_path = f\"artifacts/models/model_{now}.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cb48226",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               race_key   race_date venue win_pattern air_temp  wind_speed  \\\n",
      "0     2025-01-01-01-若 松  2025-01-01   若 松        NIGE     None         3.0   \n",
      "1     2025-01-01-02-若 松  2025-01-01   若 松        NIGE     None         2.0   \n",
      "2     2025-01-01-03-若 松  2025-01-01   若 松        NUKI     None         1.0   \n",
      "3     2025-01-01-04-若 松  2025-01-01   若 松      MAKURI     None         2.0   \n",
      "4     2025-01-01-05-若 松  2025-01-01   若 松        NIGE     None         1.0   \n",
      "...                 ...         ...   ...         ...      ...         ...   \n",
      "1577  2025-08-12-08-若 松  2025-08-12   若 松        NIGE     None         2.0   \n",
      "1578  2025-08-12-09-若 松  2025-08-12   若 松        NUKI     None         3.0   \n",
      "1579  2025-08-12-10-若 松  2025-08-12   若 松        NIGE     None         3.0   \n",
      "1580  2025-08-12-11-若 松  2025-08-12   若 松        NIGE     None         3.0   \n",
      "1581  2025-08-12-12-若 松  2025-08-12   若 松        NIGE     None         3.0   \n",
      "\n",
      "      wave_height water_temp weather_txt  wind_dir_deg  ...  lane6_starts  \\\n",
      "0            0.03       None           晴         315.0  ...          17.0   \n",
      "1            0.02       None           晴           0.0  ...          12.0   \n",
      "2            0.01       None           晴           0.0  ...           3.0   \n",
      "3            0.02       None           晴         315.0  ...          17.0   \n",
      "4            0.01       None           晴           0.0  ...          12.0   \n",
      "...           ...        ...         ...           ...  ...           ...   \n",
      "1577         0.02       None          曇り          45.0  ...          10.0   \n",
      "1578         0.03       None          曇り          45.0  ...           4.0   \n",
      "1579         0.03       None          曇り          45.0  ...           7.0   \n",
      "1580         0.03       None          曇り          45.0  ...          11.0   \n",
      "1581         0.03       None          曇り          45.0  ...           3.0   \n",
      "\n",
      "     lane6_firsts  lane6_first_rate lane6_two_rate lane6_three_rate  \\\n",
      "0             0.0          0.000000       0.058824         0.058824   \n",
      "1             0.0          0.000000       0.083333         0.166667   \n",
      "2             1.0          0.333333       0.333333         0.666667   \n",
      "3             0.0          0.000000       0.000000         0.000000   \n",
      "4             0.0          0.000000       0.000000         0.000000   \n",
      "...           ...               ...            ...              ...   \n",
      "1577          0.0          0.000000       0.000000         0.200000   \n",
      "1578          0.0          0.000000       0.000000         0.250000   \n",
      "1579          0.0          0.000000       0.000000         0.142857   \n",
      "1580          0.0          0.000000       0.090909         0.363636   \n",
      "1581          0.0          0.000000       0.000000         0.333333   \n",
      "\n",
      "      first_lane  second_lane third_lane  trifecta_odds  \\\n",
      "0              1            5          4           25.2   \n",
      "1              1            3          6           84.8   \n",
      "2              6            4          2         1517.3   \n",
      "3              5            4          2           15.4   \n",
      "4              1            2          4            2.6   \n",
      "...          ...          ...        ...            ...   \n",
      "1577           1            3          5           13.2   \n",
      "1578           5            4          6          236.1   \n",
      "1579           1            3          4           10.1   \n",
      "1580           1            4          2           11.7   \n",
      "1581           1            4          2           13.3   \n",
      "\n",
      "      trifecta_popularity_rank  \n",
      "0                         10.0  \n",
      "1                         28.0  \n",
      "2                        110.0  \n",
      "3                          3.0  \n",
      "4                          1.0  \n",
      "...                        ...  \n",
      "1577                       5.0  \n",
      "1578                      64.0  \n",
      "1579                       3.0  \n",
      "1580                       4.0  \n",
      "1581                       7.0  \n",
      "\n",
      "[1582 rows x 99 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/1279145131.py:60: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_recent = pd.read_sql(query, conn)\n",
      "/Users/keiichiro/workspace/boat_racing/model/roi_util.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[bool_cols] = df[bool_cols].fillna(False).astype(bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[simulate] Loaded 1582 rows (2025-01-01 – 2025-08-30).\n",
      "columns: race_key, race_date, venue, win_pattern, air_temp, wind_speed, wave_height, water_temp, weather_txt, wind_dir_deg, lane1_racer_id, lane1_exh_time, lane1_st, lane1_course, lane1_fs_flag, lane1_rank, lane2_racer_id, lane2_exh_time, lane2_st, lane2_course, lane2_fs_flag, lane2_rank, lane3_racer_id, lane3_exh_time, lane3_st, lane3_course, lane3_fs_flag, lane3_rank, lane4_racer_id, lane4_exh_time, lane4_st, lane4_course, lane4_fs_flag, lane4_rank, lane5_racer_id, lane5_exh_time, lane5_st, lane5_course, lane5_fs_flag, lane5_rank, lane6_racer_id, lane6_exh_time, lane6_st, lane6_course, lane6_fs_flag, lane6_rank, lane1_starts, lane1_firsts, lane1_first_rate, lane1_two_rate, lane1_three_rate, lane2_starts, lane2_firsts, lane2_first_rate, lane2_two_rate, lane2_three_rate, lane3_starts, lane3_firsts, lane3_first_rate, lane3_two_rate, lane3_three_rate, lane4_starts, lane4_firsts, lane4_first_rate, lane4_two_rate, lane4_three_rate, lane5_starts, lane5_firsts, lane5_first_rate, lane5_two_rate, lane5_three_rate, lane6_starts, lane6_firsts, lane6_first_rate, lane6_two_rate, lane6_three_rate, first_lane, second_lane, third_lane, trifecta_odds, trifecta_popularity_rank\n",
      "[compute_metrics_dataframe] Preprocessed 1582 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---- Monkey‑patch ROIAnalyzer so it uses BoatRaceDataset2 (MTL) ----------\n",
    "from types import MethodType\n",
    "from BoatRaceDataset_base import BoatRaceDatasetBase \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class _EvalDatasetMTL(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wrap BoatRaceDataset2 but return only 4 items (ctx, boats, lane_ids, ranks)\n",
    "    so that roi_util.py can stay unchanged.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.ds = BoatRaceDatasetBase(df)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    def __getitem__(self, idx):\n",
    "        ctx, boats, lane_ids, ranks, _, _ = self.ds[idx]\n",
    "        return ctx, boats, lane_ids, ranks\n",
    "\n",
    "def _create_loader_mtl(self, df_eval: pd.DataFrame):\n",
    "    \"\"\"Replacement for ROIAnalyzer._create_loader (MTL‑aware).\"\"\"\n",
    "    df = self.preprocess_df(df_eval, self.scaler, self.num_cols)\n",
    "    ds_eval = _EvalDatasetMTL(df)\n",
    "    loader = DataLoader(ds_eval, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    need_cols = [\"first_lane\", \"second_lane\", \"third_lane\"]\n",
    "    if all(c in df.columns for c in need_cols):\n",
    "        lanes_np = df[need_cols].to_numpy(dtype=np.int64) - 1\n",
    "    else:\n",
    "        # 予測テーブルでは真の1〜3着が無いのが普通。ダミー(0,1,2)で形だけ満たす\n",
    "        lanes_np = np.tile(np.array([0, 1, 2], dtype=np.int64), (len(df), 1))\n",
    "    return loader, df, lanes_np\n",
    "\n",
    "import roi_util as _roi_util_mod\n",
    "_roi_util_mod.ROIAnalyzer._create_loader = _create_loader_mtl\n",
    "\n",
    "from roi_util import ROIAnalyzer\n",
    "\n",
    "\n",
    "#  # 最新のモデルを取得\n",
    "# model_list = os.listdir(\"artifacts/models\")\n",
    "# model_list = [f for f in model_list if f.endswith(\".pth\")]\n",
    "# if model_list:\n",
    "#     latest_model = sorted(model_list)[-1]  # 最新のモデルを選択\n",
    "#     model_path = os.path.join(\"artifacts\", \"models\", latest_model)\n",
    "#     print(f\"Using latest model: {model_path}\")\n",
    "#     # モデルをロード\n",
    "#     model = DualHeadRanker(boat_in=boat_dim)\n",
    "#     model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "today = dt.date.today()\n",
    "# 2025年1月1日以降のデータを取得する場合は、以下の行を変更してください。\n",
    "start_date = dt.date(2025, 1, 1)\n",
    "# start_date = today - dt.timedelta(days=20)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM feat.eval_features_base\n",
    "    WHERE race_date BETWEEN '{start_date}' AND '{today}'\n",
    "\"\"\"\n",
    "df_recent = pd.read_sql(query, conn)\n",
    "print(df_recent)\n",
    "\n",
    "\n",
    "df_recent.drop(columns=exclude, inplace=True, errors=\"ignore\")\n",
    "\n",
    "df_recent.to_csv(\"artifacts/eval_features_recent_base.csv\", index=False)\n",
    "\n",
    "if df_recent.empty:\n",
    "    print(\"[simulate] No rows fetched for last 3 months.\")\n",
    "\n",
    "print(f\"[simulate] Loaded {len(df_recent)} rows ({start_date} – {today}).\")\n",
    "print(f\"columns: {', '.join(df_recent.columns)}\")\n",
    "\n",
    "# ---- wrap MTL model so ROIAnalyzer sees only rank scores ----\n",
    "class _RankOnly(nn.Module):\n",
    "    \"\"\"Adapter: forward() returns rank_pred tensor only, temperature-scaled.\"\"\"\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "    def forward(self, *args, **kwargs):\n",
    "        _, rank_pred, _ = self.base(*args, **kwargs)\n",
    "        return rank_pred / TEMPERATURE\n",
    "\n",
    "# ----- metrics & equity (best‑practice defaults) -----\n",
    "rank_model = _RankOnly(model).to(device)\n",
    "\n",
    "analyzer = ROIAnalyzer(model=rank_model, scaler=scaler,\n",
    "                       num_cols=NUM_COLS, device=device)\n",
    "\n",
    "df_trifecta_met = analyzer.compute_metrics_dataframe(\n",
    "    df_eval=df_recent,\n",
    "    tau=5.0,                 # ← Fractional‑Kelly倍率を上げてユニットを実用域へ\n",
    "    calibrate=\"platt\",        # ← Platt scaling で確率をキャリブレーション\n",
    "    bet_type=\"trifecta\",  # ← 三連単を対象にする\n",
    ")\n",
    "\n",
    "df_trifecta_met.to_csv(\"artifacts/metrics_trifecta.csv\", index=False)\n",
    "\n",
    "# hitが True の行だけを抽出\n",
    "df_trifecta_met_hit = df_trifecta_met[df_trifecta_met[\"hit\"] == True]\n",
    "df_trifecta_met_hit.to_csv(\"artifacts/metrics_trifecta_hit.csv\", index=False)\n",
    "\n",
    "# df_trifecta_eq = ROIAnalyzer.compute_equity_curve(\n",
    "#     df_trifecta_met,\n",
    "#     bet_unit=1,\n",
    "#     bet_mode=\"kelly\",\n",
    "#     min_conf=1.0,  # ← Platt scaling でキャリブレーション後の信頼度\n",
    "#     min_edge=0.0,\n",
    "#     min_kelly=0.00,\n",
    "#     max_units=15,\n",
    "#     use_conf_factor=True\n",
    "# )\n",
    "# df_trifecta_eq.to_csv(\"artifacts/equity_curve_trifecta.csv\", index=False)\n",
    "\n",
    "\n",
    "# ROIAnalyzer.plot_equity_curve(\n",
    "#     df_trifecta_eq,\n",
    "#     title=\"Equity Curve (Platt‑calib, τ=5, conf≥0.2 & edge≥0.3)\",\n",
    "#     use_jpy=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4915ae0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] Evaluating confidence & trifecta rank on recent predictions…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keiichiro/workspace/boat_racing/model/roi_util.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[bool_cols] = df[bool_cols].fillna(False).astype(bool)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 予測でも「自信度」と「正解三連単の順位」を評価し、CSV に記録 ---\n",
    "print(\"[predict] Evaluating confidence & trifecta rank on recent predictions…\")\n",
    "\n",
    "# ROIAnalyzer の前処理（スケーリング等）をそのまま使ってローダを作成\n",
    "loader_eval, _df_eval_proc, _lanes = analyzer._create_loader(df_recent)\n",
    "\n",
    "# 既に上で用意した rank_model は「rank_pred だけ」を返すアダプタ\n",
    "model.eval(); rank_model.eval()\n",
    "all_scores, all_ranks, all_keys = [], [], []\n",
    "\n",
    "row_ptr = 0\n",
    "with torch.no_grad():\n",
    "    for ctx, boats, lane_ids, ranks in loader_eval:\n",
    "        ctx, boats, lane_ids = ctx.to(device), boats.to(device), lane_ids.to(device)\n",
    "        scores = rank_model(ctx, boats, lane_ids)\n",
    "        B = scores.size(0)\n",
    "\n",
    "        # --- core outputs ---\n",
    "        all_scores.append(scores.cpu())\n",
    "        all_ranks.append(ranks)\n",
    "\n",
    "        # --- meta values (race_key / odds) ---\n",
    "        all_keys.extend(_df_eval_proc[\"race_key\"].iloc[row_ptr : row_ptr + B].tolist())\n",
    "        row_ptr += B\n",
    "\n",
    "\n",
    "all_scores = torch.cat(all_scores, dim=0)   # (N,6)\n",
    "all_ranks  = torch.cat(all_ranks,  dim=0)   # (N,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "902d98d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▼ Permutation importance (ALL features)\n",
      "[saved] artifacts/perm_importance_all_base.csv\n",
      "[group‑perm] Evaluating pattern groups…\n",
      "[group‑perm] Δval_nll: {'pat_group_delta': 0.0, 'lose_group_delta': 0.0, 'axis_group_delta': 0.0, 'compat_group_delta': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#  グループ Ablation: 重要列を 5～6 個まとめてドロップして val_nll を比較\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def permute_importance(model, dataset, device=\"cpu\", cols=None):\n",
    "    \"\"\"\n",
    "    Permutation importance: 各特徴量列をランダムに permute して val_nll の悪化量を調べる\n",
    "    \"\"\"\n",
    "    base_loss = evaluate_model(model, dataset, device)\n",
    "\n",
    "    # ----- 列リストを決める --------------------------------------------------\n",
    "    # cols=None なら「データフレームに存在する “使えそうな” 全列」を対象にする\n",
    "    if cols is None:\n",
    "        # 予測ターゲットやキー列は除外\n",
    "        skip = {\"race_key\", \"race_date\"}\n",
    "        # rank 列（教師信号）や欠損だらけの列も除外\n",
    "        skip |= {c for c in dataset.f.columns if c.endswith(\"_rank\")}\n",
    "        cols = [c for c in dataset.f.columns if c not in skip]\n",
    "\n",
    "    importances: dict[str, float] = {}\n",
    "    df_full = dataset.f\n",
    "\n",
    "    for col in cols:\n",
    "        # --- その列だけランダムに permute ---\n",
    "        shuffled = df_full.copy()\n",
    "        shuffled[col] = np.random.permutation(shuffled[col].values)\n",
    "        tmp_ds = BoatRaceDatasetBase(shuffled)\n",
    "        loss = evaluate_model(model, tmp_ds, device)\n",
    "        importances[col] = loss - base_loss   # 悪化分 (大 → 重要)\n",
    "    return importances\n",
    "\n",
    "def run_ablation_groups(\n",
    "    df_full: pd.DataFrame,\n",
    "    group_size: int = 6,\n",
    "    epochs: int = 5,\n",
    "    seed: int = 42,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    全特徴量をランダムに group_size 個ずつ束ね、\n",
    "    そのグループを丸ごと削除して再学習 → val_nll を返す。\n",
    "\n",
    "    戻り値: list[tuple[list[str], float]]\n",
    "        (ドロップした列リスト, val_nll) を val_nll 昇順で並べたもの\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    essential_cols = set(NUM_COLS)          # ctx 用の連続値\n",
    "    for l in range(1, 7):\n",
    "        essential_cols.update({\n",
    "            f\"lane{l}_exh_time\",\n",
    "            f\"lane{l}_st\",\n",
    "            f\"lane{l}_weight\",\n",
    "            f\"lane{l}_bf_course\",\n",
    "            f\"lane{l}_fs_flag\",\n",
    "            f\"lane{l}_racer_id\",\n",
    "            f\"lane{l}_racer_name\",\n",
    "            f\"lane{l}_racer_age\",\n",
    "            f\"lane{l}_racer_weight\",\n",
    "        })\n",
    "    # --- 対象列を決める（ターゲット & キー列は除外） ---\n",
    "    skip = {\"race_key\", \"race_date\"}\n",
    "    skip |= {c for c in df_full.columns if c.endswith(\"_rank\")}\n",
    "    skip |= essential_cols  \n",
    "    skip |= {c for c in df_full.columns if c.endswith(\"_rank\")}\n",
    "    cols = [c for c in df_full.columns if c not in skip]\n",
    "    random.shuffle(cols)\n",
    "\n",
    "    groups = [cols[i : i + group_size] for i in range(0, len(cols), group_size)]\n",
    "    results = []\n",
    "\n",
    "    latest_date = pd.to_datetime(df_full[\"race_date\"]).dt.date.max()\n",
    "    cutoff = latest_date - dt.timedelta(days=90)\n",
    "\n",
    "    for g in groups:\n",
    "        df_drop = df_full.drop(columns=g)\n",
    "\n",
    "        ds_tr = BoatRaceDatasetBase(df_drop[df_drop[\"race_date\"] < cutoff])\n",
    "        ds_va = BoatRaceDatasetBase(df_drop[df_drop[\"race_date\"] >= cutoff])\n",
    "\n",
    "        ld_tr = DataLoader(ds_tr, batch_size=256, shuffle=True)\n",
    "        ld_va = DataLoader(ds_va, batch_size=512)\n",
    "\n",
    "        model = DualHeadRanker().to(device)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-5)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            model.train()\n",
    "            for ctx, boats, lane_ids, ranks, st_true, st_mask in ld_tr:\n",
    "                ctx, boats = ctx.to(device), boats.to(device)\n",
    "                lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "                \n",
    "                st_true, st_mask = st_true.to(device), st_mask.to(device)\n",
    "                st_pred, scores = model(ctx, boats, lane_ids)\n",
    "                pl_loss = pl_nll_topk(scores, ranks, k=TOPK_K, weights=TOPK_WEIGHTS)\n",
    "                mse_st = ((st_pred - st_true) ** 2 * st_mask.float()).sum() / st_mask.float().sum()\n",
    "                loss = pl_loss + LAMBDA_ST * mse_st\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        val_loss = evaluate_model(model, ds_va, device)\n",
    "        results.append((g, val_loss))\n",
    "\n",
    "    return sorted(results, key=lambda x: x[1])  # 小さい順に重要\n",
    "\n",
    "print(\"▼ Permutation importance (ALL features)\")\n",
    "all_imp = permute_importance(model, ds_val, device)\n",
    "imp_path = \"artifacts/perm_importance_all_base.csv\"\n",
    "pd.Series(all_imp).sort_values(ascending=False).to_csv(imp_path)\n",
    "print(f\"[saved] {imp_path}\")\n",
    "\n",
    "# ---------- Group permutation importance for pattern families ----------\n",
    "from copy import deepcopy\n",
    "\n",
    "def permute_group(model, df_full: pd.DataFrame, cols: list[str], device=\"cpu\") -> float:\n",
    "    \"\"\"Return Δval_nll when permuting the given group of columns jointly.\"\"\"\n",
    "    base_ds = BoatRaceDatasetBase(df_full)\n",
    "    base_loss = evaluate_model(model, base_ds, device)\n",
    "    shuffled = df_full.copy()\n",
    "    if not cols:\n",
    "        return 0.0\n",
    "    idx = np.random.permutation(len(shuffled))\n",
    "    for c in cols:\n",
    "        if c in shuffled.columns:\n",
    "            shuffled[c] = shuffled[c].values[idx]\n",
    "    tmp_ds = BoatRaceDatasetBase(shuffled)\n",
    "    loss = evaluate_model(model, tmp_ds, device)\n",
    "    return float(loss - base_loss)\n",
    "\n",
    "# collect groups (prefer gated)\n",
    "pat_cols = []\n",
    "lose_cols = []\n",
    "for l in range(1, 7):\n",
    "    for p in [\"nige\",\"sashi\",\"makuri\",\"makurizashi\",\"nuki\",\"megumare\",\"other\"]:\n",
    "        g = f\"lane{l}_pat_{p}_rate_gated\"\n",
    "        r_ = f\"lane{l}_pat_{p}_rate\"\n",
    "        if g in result_df.columns:\n",
    "            pat_cols.append(g)\n",
    "        elif r_ in result_df.columns:\n",
    "            pat_cols.append(r_)\n",
    "for p in [\"sashi\",\"makuri\",\"makurizashi\",\"nuki\",\"penalty\"]:\n",
    "    g = f\"lane1_lose_{p}_rate_gated\"\n",
    "    r_ = f\"lane1_lose_{p}_rate\"\n",
    "    if g in result_df.columns:\n",
    "        lose_cols.append(g)\n",
    "    elif r_ in result_df.columns:\n",
    "        lose_cols.append(r_)\n",
    "\n",
    "print(\"[group‑perm] Evaluating pattern groups…\")\n",
    "# axes family (precomputed columns)\n",
    "axis_cols = []\n",
    "for l in range(1, 7):\n",
    "    for nm in [\"attack_axis\",\"chaos_axis\",\"entropy\",\"margin\"]:\n",
    "        c = f\"lane{l}_pat_{nm}\"\n",
    "        if c in result_df.columns:\n",
    "            axis_cols.append(c)\n",
    "# compat family\n",
    "compat_cols = [f\"compat_lane{l}\" for l in range(2,7) if f\"compat_lane{l}\" in result_df.columns]\n",
    "\n",
    "res = {\n",
    "    \"pat_group_delta\": permute_group(model, result_df, pat_cols, device),\n",
    "    \"lose_group_delta\": permute_group(model, result_df, lose_cols, device),\n",
    "    \"axis_group_delta\": permute_group(model, result_df, axis_cols, device) if axis_cols else 0.0,\n",
    "    \"compat_group_delta\": permute_group(model, result_df, compat_cols, device) if compat_cols else 0.0,\n",
    "}\n",
    "print(\"[group‑perm] Δval_nll:\", res)\n",
    "pd.Series(res).to_csv(\"artifacts/group_perm_pattern.csv\")\n",
    "\n",
    "# # ② グループ Ablation\n",
    "# print(\"▼ Group ablation (drop 6 cols each)\")\n",
    "# ab_results = run_ablation_groups(result_df, group_size=6,\n",
    "#                                     epochs=5, device=device)\n",
    "# abl_path = \"artifacts/ablation_results.csv\"\n",
    "# with open(abl_path, \"w\", newline=\"\") as f:\n",
    "#     import csv\n",
    "#     w = csv.writer(f); w.writerow([\"dropped_cols\", \"val_nll\"])\n",
    "#     for cols, v in ab_results:\n",
    "#         w.writerow([\"|\".join(cols), f\"{v:.6f}\"])\n",
    "# print(f\"[saved] {abl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250858ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1\n",
      "total_submit : 158200.00 JPY\n",
      "total_benefit: 191130.00 JPY\n",
      "roi : 120.82%\n",
      "n = 2\n",
      "total_submit : 316400.00 JPY\n",
      "total_benefit: 331780.00 JPY\n",
      "roi : 104.86%\n",
      "n = 3\n",
      "total_submit : 474600.00 JPY\n",
      "total_benefit: 439140.00 JPY\n",
      "roi : 92.53%\n",
      "n = 4\n",
      "total_submit : 632800.00 JPY\n",
      "total_benefit: 521860.00 JPY\n",
      "roi : 82.47%\n",
      "n = 5\n",
      "total_submit : 791000.00 JPY\n",
      "total_benefit: 605810.00 JPY\n",
      "roi : 76.59%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# all_ranksとall_scoresを結合したdfに変換\n",
    "df_scores = pd.DataFrame(all_scores.numpy(), columns=[f\"lane{i+1}_score\" for i in range(6)])\n",
    "df_ranks = pd.DataFrame(all_ranks.numpy(), columns=[f\"lane{i+1}_rank\" for i in range(6)])\n",
    "df_score_ranks = pd.concat([df_scores, df_ranks], axis=1)   \n",
    "df_score_ranks[\"race_key\"] = all_keys\n",
    "\n",
    "# df_mergedから重複行を削除\n",
    "df_score_ranks = df_score_ranks.drop_duplicates()\n",
    "\n",
    "# merge odds from df_trifecta_met_hit by race_key\n",
    "df_score_ranks = df_score_ranks.merge(df_trifecta_met_hit[[\"race_key\",\"trifecta_odds\"]], on=\"race_key\", how=\"left\")\n",
    "\n",
    "# --- lane 列をまとめて list 化 ---\n",
    "score_cols = [f\"lane{i}_score\" for i in range(1, 7)]\n",
    "rank_cols  = [f\"lane{i}_rank\"  for i in range(1, 7)]\n",
    "\n",
    "df_score_ranks[\"scores\"] = df_score_ranks[score_cols].apply(\n",
    "    lambda r: [float(x) for x in r.values.tolist()], axis=1\n",
    ")\n",
    "df_score_ranks[\"ranks\"] = df_score_ranks[rank_cols].apply(\n",
    "    lambda r: [int(x) for x in r.values.tolist()], axis=1\n",
    ")\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "def pl_true_order_prob(scores, ranks):\n",
    "    \"\"\"\n",
    "    Plackett–Luce で '真の完全着順(1→6位)' の確率を計算。\n",
    "    scores: 長さ6のスコア配列, ranks: 長さ6の真の順位 (1=最上位)\n",
    "    \"\"\"\n",
    "    w = np.exp(np.array(scores, dtype=float))\n",
    "    # 真の順序（1→2→…→6）に並んだインデックス\n",
    "    order = [i for i, _ in sorted(enumerate(ranks), key=lambda t: t[1])]\n",
    "    denom = float(w.sum())\n",
    "    p = 1.0\n",
    "    for idx in order:\n",
    "        if denom <= 0:\n",
    "            return 0.0\n",
    "        p *= float(w[idx] / denom)\n",
    "        denom -= float(w[idx])\n",
    "    return float(p)\n",
    "\n",
    "# 6! (=720) 通りの全順位\n",
    "ALL_PERMS = list(permutations(range(6), 6))\n",
    "\n",
    "def true_order_rank(scores, ranks):\n",
    "    \"\"\"\n",
    "    全 6! 通りの PL 確率で並べたとき、真の完全順位が何番目か（1始まり）。\n",
    "    \"\"\"\n",
    "    w = np.exp(np.array(scores, dtype=float))\n",
    "    denom0 = float(w.sum())\n",
    "    true_perm = tuple(i for i, _ in sorted(enumerate(ranks), key=lambda t: t[1]))\n",
    "\n",
    "    def prob_of_perm(perm):\n",
    "        denom = denom0\n",
    "        p = 1.0\n",
    "        for idx in perm:\n",
    "            if denom <= 0:\n",
    "                return 0.0\n",
    "            p *= float(w[idx] / denom)\n",
    "            denom -= float(w[idx])\n",
    "        return p\n",
    "\n",
    "    probs = [(perm, prob_of_perm(perm)) for perm in ALL_PERMS]\n",
    "    probs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for k, (perm, _) in enumerate(probs, start=1):\n",
    "        if perm == true_perm:\n",
    "            return k\n",
    "    return len(probs) + 1  # 通常は到達しない\n",
    "\n",
    "# 列の追加\n",
    "df_score_ranks[\"true_order_prob\"] = df_score_ranks.apply(\n",
    "    lambda row: pl_true_order_prob(row[\"scores\"], row[\"ranks\"]), axis=1\n",
    ")\n",
    "df_score_ranks[\"true_order_rank\"] = df_score_ranks.apply(\n",
    "    lambda row: true_order_rank(row[\"scores\"], row[\"ranks\"]), axis=1\n",
    ")\n",
    "\n",
    "# 保存\n",
    "df_score_ranks.to_csv(\"artifacts/merged_scores_ranks_base.csv\", index=False)\n",
    "\n",
    "# df_score_ranksを行でループ\n",
    "total_benefit = 0.0\n",
    "total_submit = 0.0\n",
    "\n",
    "for n in range(1, 6):\n",
    "    # ★ 各 n でリセット\n",
    "    total_submit = 0.0\n",
    "    total_benefit = 0.0\n",
    "\n",
    "    for _, row in df_score_ranks.iterrows():\n",
    "        total_submit += 100 * n\n",
    "        odds = row.get(\"trifecta_odds\", None)\n",
    "        true_rank = row.get(\"true_order_rank\", None)\n",
    "\n",
    "        if true_rank is not None and true_rank <= n:\n",
    "            # ★ 欠損オッズは0扱い\n",
    "            if pd.isna(odds):\n",
    "                odds = 0.0\n",
    "            total_benefit += float(odds) * 100\n",
    "\n",
    "    roi = ((total_benefit / total_submit) * 100) if total_submit > 0 else float(\"nan\")\n",
    "    print(f\"n = {n}\")\n",
    "    print(f\"total_submit : {total_submit:.2f} JPY\")\n",
    "    print(f\"total_benefit: {total_benefit:.2f} JPY\")\n",
    "    print(f\"roi : {roi:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6c41a04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[predict] N=1582\n",
      "  • Top‑1 Acc              : 0.655   (baseline 0.554)\n",
      "  • Pos1/2/3 Acc           : 0.655/0.366/0.304 (baseline 0.554/0.232/0.212)\n",
      "  • Top‑3 unordered Hit    : 0.349   (baseline 0.169)\n",
      "  • Trifecta Hit           : 0.159   (baseline 0.074)\n",
      "  • Winner MRR             : 0.795\n",
      "  • Spearman ρ             : 0.626\n",
      "  • Score variance (mean/median): 1.9687 / 1.9181\n",
      "  • Avg rank of true trifecta (unordered) : 3.49\n",
      "  • Avg rank of true trifecta (strict)    : 14.14\n",
      "gain: pop_rank − model_rank\n",
      "  • Pop vs Model (ordered true trifecta rank): mean pop 18.75, avg gain 4.62, median gain 1.50, beat-rate 0.550\n",
      "  • TopN cover (ordered): N=1 model 0.160 vs pop 0.103 (Δ0.057); N=3 model 0.335 vs pop 0.247 (Δ0.087)\n",
      "[saved] artifacts/predict_metrics_recent.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def top1_accuracy(scores: torch.Tensor, ranks: torch.Tensor) -> float:\n",
    "    \"\"\"Top‑1 accuracy: predicted winner vs true winner (ranks: 1 is best).\"\"\"\n",
    "    pred_top1 = scores.argmax(dim=1)        # (B,)\n",
    "    true_top1 = ranks.argmin(dim=1)         # (B,)\n",
    "    return (pred_top1 == true_top1).float().mean().item()\n",
    "\n",
    "def trifecta_hit_rate(scores: torch.Tensor, ranks: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    予測スコア上位3艇の順番が、実際の1〜3着と完全一致する割合。\n",
    "    \"\"\"\n",
    "    pred_top3 = torch.topk(scores, k=3, dim=1).indices\n",
    "    true_top3 = torch.topk(-ranks, k=3, dim=1).indices  # 小さい順に 1→3 着\n",
    "    hit = [p.tolist() == t.tolist() for p, t in zip(pred_top3, true_top3)]\n",
    "    return float(sum(hit) / len(hit)) if len(hit) else float(\"nan\")\n",
    "\n",
    "def constant_123_trifecta_hit(ranks: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Hit‑rate when always predicting trifecta 1‑2‑3 in order.\n",
    "    \"\"\"\n",
    "    true_top3 = torch.topk(-ranks, k=3, dim=1).indices   # (B,3)\n",
    "    baseline  = torch.tensor([0, 1, 2], dtype=torch.long, device=ranks.device)\n",
    "    return (true_top3 == baseline).all(dim=1).float().mean().item()\n",
    "\n",
    "def baseline123_position_accuracy(ranks: torch.Tensor, pos: int) -> float:\n",
    "    \"\"\"\n",
    "    Baseline per‑position accuracy when assuming boat pos finishes pos‑th.\n",
    "    pos ∈ {1,2,3}\n",
    "    \"\"\"\n",
    "    true_idx = (ranks == pos).float().argmax(dim=1)          # (B,)\n",
    "    baseline_idx = torch.tensor(pos - 1, dtype=torch.long, device=ranks.device)\n",
    "    return (true_idx == baseline_idx).float().mean().item()\n",
    "\n",
    "def baseline123_top3_unordered_hit(ranks: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Order‑agnostic hit‑rate when always predicting the set {1,2,3}.\n",
    "    \"\"\"\n",
    "    true_top3 = torch.topk(-ranks, k=3, dim=1).indices\n",
    "    hit = [set(t.tolist()) == {0,1,2} for t in true_top3]\n",
    "    return float(sum(hit) / len(hit)) if len(hit) else float(\"nan\")\n",
    "\n",
    "def get_trifecta_rank_unordered(scores: torch.Tensor, true_ranks: torch.Tensor) -> list[int]:\n",
    "    \"\"\"真の三連複（順序なし）集合が、全20集合の中で何番目か（1始まり）\"\"\"\n",
    "    from itertools import combinations\n",
    "    combos = list(combinations(range(6), 3))\n",
    "    res: list[int] = []\n",
    "    for sc, tr in zip(scores, true_ranks):\n",
    "        true_set = {i for i, r in enumerate(tr.tolist()) if r <= 3}\n",
    "        combo_scores = [(c, sc[list(c)].sum().item()) for c in combos]\n",
    "        combo_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        for idx, (c, _) in enumerate(combo_scores, start=1):\n",
    "            if set(c) == true_set:\n",
    "                res.append(idx)\n",
    "                break\n",
    "        else:\n",
    "            res.append(len(combos) + 1)\n",
    "    return res\n",
    "\n",
    "def get_trifecta_rank_ordered(scores: torch.Tensor, true_ranks: torch.Tensor) -> list[int]:\n",
    "    \"\"\"真の三連単（順序あり）が、PL確率で並べた全120順列の中で何番目か（1始まり）。\"\"\"\n",
    "    import itertools\n",
    "    perms = list(itertools.permutations(range(6), 3))\n",
    "    res: list[int] = []\n",
    "    for sc, tr in zip(scores, true_ranks):\n",
    "        # 数値安定化: 行内の最大を引いてから exp\n",
    "        es = torch.exp(sc - sc.max())\n",
    "        ordered_true = sorted(range(6), key=lambda i: tr[i].item())[:3]\n",
    "\n",
    "        denom0 = es.sum().item()\n",
    "        perm_probs = []\n",
    "        for p0, p1, p2 in perms:\n",
    "            d1 = denom0\n",
    "            d2 = d1 - es[p0].item()\n",
    "            d3 = d2 - es[p1].item()\n",
    "            if d2 <= 0 or d3 <= 0:\n",
    "                prob = 0.0\n",
    "            else:\n",
    "                prob = (es[p0] / d1) * (es[p1] / d2) * (es[p2] / d3)\n",
    "            perm_probs.append(((p0, p1, p2), float(prob)))\n",
    "\n",
    "        perm_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "        for idx, (p, _) in enumerate(perm_probs, start=1):\n",
    "            if list(p) == ordered_true:\n",
    "                res.append(idx)\n",
    "                break\n",
    "        else:\n",
    "            res.append(len(perms) + 1)\n",
    "    return res\n",
    "\n",
    "def top3_unordered_hit_rate(scores: torch.Tensor, ranks: torch.Tensor) -> float:\n",
    "    pred_top3 = torch.topk(scores, k=3, dim=1).indices\n",
    "    true_top3 = torch.topk(-ranks, k=3, dim=1).indices\n",
    "    hit = [(set(p.tolist()) == set(t.tolist())) for p, t in zip(pred_top3, true_top3)]\n",
    "    return float(sum(hit) / len(hit)) if len(hit) else float(\"nan\")\n",
    "\n",
    "def mean_reciprocal_rank(scores: torch.Tensor, ranks: torch.Tensor) -> float:\n",
    "    order = scores.argsort(dim=1, descending=True)          # (B,6)\n",
    "    true_winner_idx = ranks.argmin(dim=1)                   # (B,)\n",
    "    # position of true winner in each row (1-based)\n",
    "    pos = (order == true_winner_idx[:, None]).float().argmax(dim=1) + 1\n",
    "    return (1.0 / pos.float()).mean().item()\n",
    "\n",
    "def spearman_corr(scores: torch.Tensor, ranks: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Average per‑race Spearman rank correlation between predicted and true rank orders.\n",
    "    \"\"\"\n",
    "    pred_rank = scores.argsort(dim=1, descending=True).argsort(dim=1).float() + 1  # 1..6\n",
    "    true_rank = ranks.float()\n",
    "    d = pred_rank - true_rank\n",
    "    rho = 1 - 6 * (d ** 2).sum(dim=1) / (6 * (6**2 - 1))\n",
    "    return rho.mean().item()\n",
    "\n",
    "score_vars = all_scores.var(dim=1)\n",
    "tri_ranks  = get_trifecta_rank_unordered(all_scores, all_ranks)\n",
    "tri_ranks_order = get_trifecta_rank_ordered(all_scores, all_ranks)\n",
    "mean_tri_order  = float(np.mean(tri_ranks_order)) if len(tri_ranks_order) else float(\"nan\")\n",
    "\n",
    "# ---- Popularity vs Model comparison (ordered trifecta) ----\n",
    "# We expect df_recent to carry 'trifecta_popularity_rank' per race_key (ordered).\n",
    "pop_col = None\n",
    "for c in [\"trifecta_popularity_rank\"]:\n",
    "    if c in df_recent.columns:\n",
    "        pop_col = c; break\n",
    "\n",
    "if pop_col is not None:\n",
    "    _model_rank_df = pd.DataFrame({\n",
    "        \"race_key\": np.array(all_keys),\n",
    "        \"model_trifecta_rank_ordered\": tri_ranks_order\n",
    "    })\n",
    "    _pop_df = df_recent[[\"race_key\", pop_col]].drop_duplicates(\"race_key\").rename(columns={pop_col: \"pop_trifecta_rank_ordered\"})\n",
    "    _cmp_df = _model_rank_df.merge(_pop_df, on=\"race_key\", how=\"inner\")\n",
    "    _cmp_df[\"pop_trifecta_rank_ordered\"] = pd.to_numeric(_cmp_df[\"pop_trifecta_rank_ordered\"], errors=\"coerce\")\n",
    "    _cmp_df = _cmp_df.dropna(subset=[\"pop_trifecta_rank_ordered\"])  # keep valid rows only\n",
    "\n",
    "    # Gains: positive if model ranks the true trifecta higher (smaller rank) than popularity\n",
    "    _cmp_df[\"gain\"] = _cmp_df[\"pop_trifecta_rank_ordered\"] - _cmp_df[\"model_trifecta_rank_ordered\"]\n",
    "\n",
    "    pop_tri_rank_mean = float(_cmp_df[\"pop_trifecta_rank_ordered\"].mean()) if len(_cmp_df) else float(\"nan\")\n",
    "    model_vs_pop_avg_gain = float(_cmp_df[\"gain\"].mean()) if len(_cmp_df) else float(\"nan\")\n",
    "    model_vs_pop_median_gain = float(_cmp_df[\"gain\"].median()) if len(_cmp_df) else float(\"nan\")\n",
    "    model_beats_pop_rate = float((_cmp_df[\"gain\"] > 0).mean()) if len(_cmp_df) else float(\"nan\")\n",
    "\n",
    "    def _cov(N: int):\n",
    "        if len(_cmp_df) == 0:\n",
    "            return float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "        pop_hit = float((_cmp_df[\"pop_trifecta_rank_ordered\"] <= N).mean())\n",
    "        model_hit = float((_cmp_df[\"model_trifecta_rank_ordered\"] <= N).mean())\n",
    "        return pop_hit, model_hit, model_hit - pop_hit\n",
    "\n",
    "    pop_top1, model_top1_ord, diff_top1 = _cov(1)\n",
    "    pop_top3, model_top3_ord, diff_top3 = _cov(3)\n",
    "else:\n",
    "    pop_tri_rank_mean = model_vs_pop_avg_gain = model_vs_pop_median_gain = model_beats_pop_rate = float(\"nan\")\n",
    "    pop_top1 = model_top1_ord = diff_top1 = pop_top3 = model_top3_ord = diff_top3 = float(\"nan\")\n",
    "    print(\"[pop] Column 'trifecta_popularity_rank' not found; skipping pop vs model comparison.\")\n",
    "\n",
    "acc_top1   = top1_accuracy(all_scores, all_ranks)\n",
    "acc_tri3   = trifecta_hit_rate(all_scores, all_ranks)\n",
    "mean_var   = score_vars.mean().item()\n",
    "median_var = score_vars.median().item()\n",
    "mean_tri   = float(np.mean(tri_ranks)) if len(tri_ranks) else float(\"nan\")\n",
    "\n",
    "# ---- compute new metrics ----\n",
    "hit_top3_unordered = top3_unordered_hit_rate(all_scores, all_ranks)\n",
    "mrr_winner        = mean_reciprocal_rank(all_scores, all_ranks)\n",
    "rho_spearman      = spearman_corr(all_scores, all_ranks)\n",
    "\n",
    "# ---- baseline metrics (constant 1‑2‑3) ----\n",
    "tri123_hit      = constant_123_trifecta_hit(all_ranks)\n",
    "base_pos1       = baseline123_position_accuracy(all_ranks, 1)\n",
    "base_pos2       = baseline123_position_accuracy(all_ranks, 2)\n",
    "base_pos3       = baseline123_position_accuracy(all_ranks, 3)\n",
    "base_top1       = base_pos1                                   # same as pos1\n",
    "base_top3_unord = baseline123_top3_unordered_hit(all_ranks)\n",
    "\n",
    "# --- per-position accuracy (model) ---\n",
    "def position_accuracy(ranks: torch.Tensor, scores: torch.Tensor, pos: int) -> float:\n",
    "    \"\"\"\n",
    "    Accuracy for predicting which boat finishes pos‑th.\n",
    "    \"\"\"\n",
    "    # Model's prediction: which boat is pos-th in predicted ranking\n",
    "    pred_rank = scores.argsort(dim=1, descending=True).argsort(dim=1) + 1\n",
    "    pred_idx = (pred_rank == pos).float().argmax(dim=1)\n",
    "    true_idx = (ranks == pos).float().argmax(dim=1)\n",
    "    return (pred_idx == true_idx).float().mean().item()\n",
    "\n",
    "acc_pos1 = position_accuracy(all_ranks, all_scores, 1)\n",
    "acc_pos2 = position_accuracy(all_ranks, all_scores, 2)\n",
    "acc_pos3 = position_accuracy(all_ranks, all_scores, 3)\n",
    "\n",
    "print(f\"[predict] N={len(all_scores)}\")\n",
    "print(f\"  • Top‑1 Acc              : {acc_top1:.3f}   (baseline {base_top1:.3f})\")\n",
    "print(f\"  • Pos1/2/3 Acc           : {acc_pos1:.3f}/{acc_pos2:.3f}/{acc_pos3:.3f} \"\n",
    "      f\"(baseline {base_pos1:.3f}/{base_pos2:.3f}/{base_pos3:.3f})\")\n",
    "print(f\"  • Top‑3 unordered Hit    : {hit_top3_unordered:.3f}   (baseline {base_top3_unord:.3f})\")\n",
    "print(f\"  • Trifecta Hit           : {acc_tri3:.3f}   (baseline {tri123_hit:.3f})\")\n",
    "print(f\"  • Winner MRR             : {mrr_winner:.3f}\")\n",
    "print(f\"  • Spearman ρ             : {rho_spearman:.3f}\")\n",
    "print(f\"  • Score variance (mean/median): {mean_var:.4f} / {median_var:.4f}\")\n",
    "print(f\"  • Avg rank of true trifecta (unordered) : {mean_tri:.2f}\")\n",
    "print(f\"  • Avg rank of true trifecta (strict)    : {mean_tri_order:.2f}\")\n",
    "print(f'gain: pop_rank − model_rank')\n",
    "print(f\"  • Pop vs Model (ordered true trifecta rank): mean pop {pop_tri_rank_mean:.2f}, avg gain {model_vs_pop_avg_gain:.2f}, median gain {model_vs_pop_median_gain:.2f}, beat-rate {model_beats_pop_rate:.3f}\")\n",
    "print(f\"  • TopN cover (ordered): N=1 model {model_top1_ord:.3f} vs pop {pop_top1:.3f} (Δ{diff_top1:.3f}); N=3 model {model_top3_ord:.3f} vs pop {pop_top3:.3f} (Δ{diff_top3:.3f})\")\n",
    "\n",
    "# ---- CSV に追記保存 ----\n",
    "import csv, os\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "metrics_path = \"artifacts/predict_metrics_recent.csv\"\n",
    "write_header = not os.path.exists(metrics_path)\n",
    "with open(metrics_path, \"a\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if write_header:\n",
    "        w.writerow([\"date\", \"n_races\",\n",
    "                    \"top1_acc\", \"pos1_acc\", \"pos2_acc\", \"pos3_acc\",\n",
    "                    \"top3unordered_hit\", \"trifecta_hit\",\n",
    "                    \"baseline123_hit\", \"baseline123_top1\",\n",
    "                    \"baseline123_pos1\", \"baseline123_pos2\", \"baseline123_pos3\",\n",
    "                    \"baseline123_top3unordered\",\n",
    "                    \"winner_mrr\", \"spearman_rho\",\n",
    "                    \"var_mean\", \"var_median\", \"tri_rank_mean\",\n",
    "                    \"tri_rank_order_mean\",\n",
    "                    \"pop_tri_rank_mean\", \"model_vs_pop_avg_gain\", \"model_vs_pop_median_gain\", \"model_beats_pop_rate\",\n",
    "                    \"pop_top1_hit\", \"model_top1_ord_hit\", \"delta_top1\",\n",
    "                    \"pop_top3_hit\", \"model_top3_ord_hit\", \"delta_top3\"])\n",
    "    w.writerow([str(today), len(all_scores),\n",
    "                acc_top1, acc_pos1, acc_pos2, acc_pos3,\n",
    "                hit_top3_unordered, acc_tri3,\n",
    "                tri123_hit, base_top1,\n",
    "                base_pos1, base_pos2, base_pos3,\n",
    "                base_top3_unord,\n",
    "                mrr_winner, rho_spearman,\n",
    "                mean_var, median_var, mean_tri, mean_tri_order,\n",
    "                pop_tri_rank_mean, model_vs_pop_avg_gain, model_vs_pop_median_gain, model_beats_pop_rate,\n",
    "                pop_top1, model_top1_ord, diff_top1,\n",
    "                pop_top3, model_top3_ord, diff_top3])\n",
    "print(f\"[saved] {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d2c3f1a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cond] 条件別の当たりやすさ分析を開始…\n",
      "[cond] 上位の条件例 (top_n=3, ROI順 上位10)\n",
      "      condition               bin  n_races  hit_rate       roi  \\\n",
      "28  pl_prob_bin     (0.142, 0.36]    394.0  0.124365  0.447039   \n",
      "32      gap_bin   (0.0264, 0.189]    395.0  0.136709  0.410295   \n",
      "29  pl_prob_bin    (0.112, 0.142]    394.0  0.126904  0.172758   \n",
      "36     wind_bin        [2.0, 4.0)    584.0  0.101027  0.038699   \n",
      "40     wave_bin       [-inf, 0.5)   1578.0  0.094423 -0.072370   \n",
      "41        venue               若 松   1578.0  0.094423 -0.072370   \n",
      "37     wind_bin        [4.0, 6.0)    465.0  0.105376 -0.081935   \n",
      "38     wind_bin        [6.0, 8.0)    138.0  0.057971 -0.093961   \n",
      "33      gap_bin  (0.0128, 0.0264]    395.0  0.091139 -0.180759   \n",
      "39     wind_bin       [-inf, 2.0)    365.0  0.084932 -0.202831   \n",
      "\n",
      "    avg_odds_on_hits  top_n  \n",
      "28         34.906122      3  \n",
      "32         30.948148      3  \n",
      "29         27.724000      3  \n",
      "36         30.844068      3  \n",
      "40         29.472483      3  \n",
      "41         29.472483      3  \n",
      "37         26.136735      3  \n",
      "38         46.887500      3  \n",
      "33         26.966667      3  \n",
      "39         28.158065      3  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/2565124370.py:148: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === 条件別ヒット率/ROI 分析（修正版） =========================\n",
    "# 目的: 「どんな条件のときに当たりやすいか？」を、ヒット率とROIで可視化\n",
    "# 入力: df_score_ranks（race_key, trifecta_odds, true_order_rank を含む）\n",
    "#       df_recent（環境・会場などの特徴）\n",
    "# 依存: analyzer, model, rank_model, device がスコープに存在する想定\n",
    "# 出力: artifacts/cond_base_table.csv, artifacts/cond_hit_roi.csv\n",
    "# 重要: 意思決定に使う条件からは “事後情報” の疑いがあるもの（例: trifecta_odds_bin）を除外\n",
    "# -------------------------------------------------------------\n",
    "import os\n",
    "import math\n",
    "from itertools import permutations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"[cond] 条件別の当たりやすさ分析を開始…\")\n",
    "\n",
    "# 0) ベース表の構築（分析に使う列をまとめる）\n",
    "_base = df_score_ranks.copy()\n",
    "_base = _base[_base[\"race_key\"].notna()].copy()\n",
    "_base[\"true_order_rank\"] = pd.to_numeric(_base[\"true_order_rank\"], errors=\"coerce\")\n",
    "_base[\"trifecta_odds\"] = pd.to_numeric(_base[\"trifecta_odds\"], errors=\"coerce\")\n",
    "\n",
    "# df_recent から環境や会場などの列をマージ（存在する列だけ）\n",
    "_cand_cols = [\n",
    "    \"race_key\", \"venue\", \"air_temp\", \"water_temp\",\n",
    "    \"wind_speed\", \"wave_height\", \"wind_dir_deg\", \"wind_sin\", \"wind_cos\",\n",
    "]\n",
    "_exist_cols = [c for c in _cand_cols if c in df_recent.columns]\n",
    "if _exist_cols:\n",
    "    _env = df_recent[_exist_cols].drop_duplicates(\"race_key\")\n",
    "    _base = _base.merge(_env, on=\"race_key\", how=\"left\")\n",
    "\n",
    "# 1) 予測スコア由来の「自信度」特徴を付与（PLのtop1確率・top2とのギャップ等）\n",
    "try:\n",
    "    _scores_mat = all_scores.detach().cpu().numpy()  # (N,6)\n",
    "    _rk_seq = _df_eval_proc[\"race_key\"].to_numpy()\n",
    "except Exception as e:\n",
    "    print(\"[cond] all_scores が見つからない/使えないため再計算します:\", e)\n",
    "    loader_eval, _df_eval_proc, _ = analyzer._create_loader(df_recent)\n",
    "    model.eval(); rank_model.eval()\n",
    "    _sc_list = []\n",
    "    with torch.no_grad():\n",
    "        for ctx, boats, lane_ids, _ranks in loader_eval:\n",
    "            ctx, boats, lane_ids = ctx.to(device), boats.to(device), lane_ids.to(device)\n",
    "            _sc = rank_model(ctx, boats, lane_ids)\n",
    "            _sc_list.append(_sc.cpu())\n",
    "    all_scores = torch.cat(_sc_list, dim=0)\n",
    "    _scores_mat = all_scores.detach().cpu().numpy()\n",
    "    _rk_seq = _df_eval_proc[\"race_key\"].to_numpy()\n",
    "\n",
    "_scores_df = pd.DataFrame(_scores_mat, columns=[f\"s{i}\" for i in range(6)])\n",
    "_scores_df[\"race_key\"] = _rk_seq\n",
    "_perms = list(permutations(range(6), 3))\n",
    "\n",
    "def _pl_feats_from_scores(row):\n",
    "    \"\"\"数値安定化したsoftmax + Plackett–Luce近似で top1/top2 確率とギャップ等を算出\"\"\"\n",
    "    s = np.array([row[f\"s{i}\"] for i in range(6)], dtype=float)\n",
    "    # 数値安定化：log-sum-exp（最大値でシフト）\n",
    "    s_ = s - np.max(s)\n",
    "    es = np.exp(s_)\n",
    "    denom0 = es.sum()\n",
    "\n",
    "    # lane softmax のエントロピー（低いほど確信強）\n",
    "    p = es / max(denom0, 1e-12)\n",
    "    entropy = float(-(p * np.log(p + 1e-12)).sum())\n",
    "    var = float(np.var(s))\n",
    "\n",
    "    # 全120通りのPL確率から top1 / top2 とギャップ\n",
    "    best1p, best2p = -1.0, -1.0\n",
    "    best1 = None\n",
    "    for a, b, c in _perms:  # 120通り\n",
    "        d2 = denom0 - es[a]\n",
    "        d3 = d2 - es[b]\n",
    "        if d2 <= 0 or d3 <= 0:\n",
    "            continue\n",
    "        prob = (es[a]/denom0) * (es[b]/d2) * (es[c]/d3)  # Plackett–Luce\n",
    "        if prob > best1p:\n",
    "            best2p = best1p\n",
    "            best1p = float(prob)\n",
    "            best1 = (a, b, c)\n",
    "        elif prob > best2p:\n",
    "            best2p = float(prob)\n",
    "\n",
    "    gap = best1p - best2p if best2p >= 0 else np.nan\n",
    "    top1_str = f\"{best1[0]+1}-{best1[1]+1}-{best1[2]+1}\" if best1 is not None else np.nan\n",
    "    return pd.Series({\n",
    "        \"pl_top1_prob\": best1p,\n",
    "        \"pl_top2_prob\": best2p,\n",
    "        \"pl_gap\": gap,\n",
    "        \"pl_top1\": top1_str,\n",
    "        \"score_entropy\": entropy,\n",
    "        \"score_var\": var,\n",
    "    })\n",
    "\n",
    "_pl_feats = _scores_df.apply(_pl_feats_from_scores, axis=1)\n",
    "_scores_df = pd.concat([_scores_df[[\"race_key\"]], _pl_feats], axis=1)\n",
    "_base = _base.merge(_scores_df, on=\"race_key\", how=\"left\")\n",
    "\n",
    "# 2) 条件のビニング\n",
    "def _safe_qcut(series, q):\n",
    "    try:\n",
    "        return pd.qcut(series, q=q, duplicates=\"drop\")\n",
    "    except Exception:\n",
    "        return pd.Series([np.nan] * len(series), index=series.index)\n",
    "\n",
    "# ※ 意思決定に使う条件からは “事後情報” の trifecta_odds_bin を除外\n",
    "if \"pl_top1_prob\" in _base.columns:\n",
    "    _base[\"pl_prob_bin\"] = _safe_qcut(_base[\"pl_top1_prob\"], 4)\n",
    "if \"pl_gap\" in _base.columns:\n",
    "    _base[\"gap_bin\"] = _safe_qcut(_base[\"pl_gap\"], 4)\n",
    "if \"wind_speed\" in _base.columns:\n",
    "    _base[\"wind_bin\"] = pd.cut(_base[\"wind_speed\"], bins=[-np.inf, 2, 4, 6, 8, np.inf], right=False)\n",
    "if \"wave_height\" in _base.columns:\n",
    "    _base[\"wave_bin\"] = pd.cut(_base[\"wave_height\"], bins=[-np.inf, 0.5, 1.0, 2.0, np.inf], right=False)\n",
    "if \"wind_sin\" in _base.columns:\n",
    "    _base[\"tailwind\"] = _base[\"wind_sin\"] < 0  # True=追い風（sin<0）\n",
    "\n",
    "# 3) 条件ごとのヒット率/ROI を「レース単位」で集計\n",
    "def _summarize_by(col, top_n, min_races=30):\n",
    "    \"\"\"各レース=1票 として評価。コスト= top_n/レース、払戻= 的中レースの trifecta_odds 合計。\"\"\"\n",
    "    if col not in _base.columns:\n",
    "        return pd.DataFrame()\n",
    "    df = _base.dropna(subset=[col, \"true_order_rank\", \"trifecta_odds\", \"race_key\"]).copy()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # グループ毎に race_key でユニーク化（複製があっても1レース1回の評価にする）\n",
    "    def _agg_group(g):\n",
    "        rep = g.drop_duplicates(\"race_key\")\n",
    "        n_races = rep[\"race_key\"].nunique()\n",
    "        # 的中判定（「上位top_nに真の順序が入っていたか」）\n",
    "        hits_mask = rep[\"true_order_rank\"] <= top_n\n",
    "        hit_rate = float(hits_mask.mean())\n",
    "        # 払戻（的中レースだけカウント）\n",
    "        total_return = float(rep.loc[hits_mask, \"trifecta_odds\"].sum())\n",
    "        cost = n_races * top_n\n",
    "        roi = (total_return - cost) / cost if cost > 0 else np.nan\n",
    "        avg_odds_on_hits = float(rep.loc[hits_mask, \"trifecta_odds\"].mean()) if hits_mask.any() else np.nan\n",
    "        return pd.Series({\n",
    "            \"n_races\": n_races,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"roi\": roi,\n",
    "            \"avg_odds_on_hits\": avg_odds_on_hits,\n",
    "        })\n",
    "\n",
    "    out = df.groupby(col, dropna=False).apply(_agg_group).reset_index()\n",
    "    out = out.rename(columns={col: \"bin\"})\n",
    "    if out.empty:\n",
    "        return out\n",
    "\n",
    "    # 表示互換のため n も付ける（= n_races）\n",
    "    out[\"n\"] = out[\"n_races\"]\n",
    "    out[\"top_n\"] = top_n\n",
    "    out[\"condition\"] = col\n",
    "    out = out[out[\"n_races\"] >= min_races].sort_values([\"roi\", \"hit_rate\"], ascending=False)\n",
    "    return out\n",
    "\n",
    "# 解析対象列（意思決定用の条件のみ）\n",
    "_cols_to_try = [\"pl_prob_bin\", \"gap_bin\", \"wind_bin\", \"wave_bin\", \"tailwind\"]\n",
    "if \"venue\" in _base.columns:\n",
    "    _cols_to_try.append(\"venue\")\n",
    "\n",
    "_tables = []\n",
    "for _n in [1, 2, 3, 4, 5]:\n",
    "    for _c in _cols_to_try:\n",
    "        if _c in _base.columns:\n",
    "            _t = _summarize_by(_c, _n, min_races=30 if _c != \"venue\" else 50)\n",
    "            if not _t.empty:\n",
    "                _tables.append(_t)\n",
    "\n",
    "_cond_result = pd.concat(_tables, ignore_index=True) if _tables else pd.DataFrame()\n",
    "\n",
    "# 4) 保存\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "_base.to_csv(\"artifacts/cond_base_table.csv\", index=False)\n",
    "_cond_result.to_csv(\"artifacts/cond_hit_roi.csv\", index=False)\n",
    "\n",
    "# 5) コンソールにハイライト表示\n",
    "if not _cond_result.empty:\n",
    "    with pd.option_context(\"display.max_rows\", 20, \"display.max_colwidth\", 60):\n",
    "        print(\"[cond] 上位の条件例 (top_n=3, ROI順 上位10)\")\n",
    "        cols_to_show = [\"condition\", \"n\", \"hit_rate\", \"roi\", \"avg_odds_on_hits\", \"top_n\"]\n",
    "        # 必要に応じて各条件固有のビン列名を追記\n",
    "        extra_cols = []\n",
    "        # 条件ごとのビン列（表示があれば自動で含める）\n",
    "        for c in [\"pl_prob_bin\", \"gap_bin\", \"wind_bin\", \"wave_bin\", \"tailwind\", \"venue\"]:\n",
    "            if c in _cond_result[\"condition\"].unique():\n",
    "                extra_cols.append(c)\n",
    "        show_top = 3\n",
    "        df_view = _cond_result.query(f\"top_n == {show_top}\").sort_values(\"roi\", ascending=False)\n",
    "        # 少なくとも代表的なカラムが出るように調整\n",
    "        print(df_view[[\"condition\", \"bin\", \"n_races\", \"hit_rate\", \"roi\", \"avg_odds_on_hits\", \"top_n\"]].head(10))\n",
    "else:\n",
    "    print(\"[cond] 条件別集計を作成できませんでした（対象列やデータ不足）。\")\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42fe33b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9n/_65_h0_d791gcmmvjcjjkn9r0000gn/T/ipykernel_69611/496614104.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_recent = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n    SELECT * FROM pred.features_with_record\n    WHERE race_date BETWEEN '2025-08-09' AND '2025-08-30'\n': relation \"pred.features_with_record\" does not exist\nLINE 2:     SELECT * FROM pred.features_with_record\n                          ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/io/sql.py:2664\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2664\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mUndefinedTable\u001b[0m: relation \"pred.features_with_record\" does not exist\nLINE 2:     SELECT * FROM pred.features_with_record\n                          ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    SELECT * FROM pred.features_with_record\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    WHERE race_date BETWEEN \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AND \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m conn \u001b[38;5;241m=\u001b[39m psycopg2\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDB_CONF)\n\u001b[0;32m---> 16\u001b[0m df_recent \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_recent)\n\u001b[1;32m     18\u001b[0m df_recent\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martifacts/pred_features_recent.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/io/sql.py:708\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/io/sql.py:2728\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2719\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2726\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2727\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2728\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2729\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/boat_racing-zew2npIb/lib/python3.9/site-packages/pandas/io/sql.py:2676\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2676\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\n    SELECT * FROM pred.features_with_record\n    WHERE race_date BETWEEN '2025-08-09' AND '2025-08-30'\n': relation \"pred.features_with_record\" does not exist\nLINE 2:     SELECT * FROM pred.features_with_record\n                          ^\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# prediction\n",
    "from roi_util import ROIPredictor\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "today = dt.date.today()\n",
    "# 2025年1月1日以降のデータを取得する場合は、以下の行を変更してください。\n",
    "start_date = dt.date(2025, 8, 9)\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT * FROM pred.features_with_record\n",
    "    WHERE race_date BETWEEN '{start_date}' AND '{today}'\n",
    "\"\"\"\n",
    "\n",
    "conn = psycopg2.connect(**DB_CONF)\n",
    "df_recent = pd.read_sql(query, conn)\n",
    "print(df_recent)\n",
    "df_recent.to_csv(\"artifacts/pred_features_recent.csv\", index=False)\n",
    "\n",
    "\n",
    "df_recent.drop(columns=exclude, inplace=True, errors=\"ignore\")\n",
    "\n",
    "df_recent = add_gate_features(df_recent, K=20.0, K_lose=20.0)\n",
    "print(\"[gate] nonzero check (pred):\", float(df_recent.filter(regex=r\"(_gate$|_gated$)\").select_dtypes(include=[\"number\"]).sum().sum()))\n",
    "df_recent = prune_to_dataset_used(df_recent)\n",
    "\n",
    "if df_recent.empty:\n",
    "    print(\"[predict] No rows fetched for the specified period.\")\n",
    "\n",
    "print(f\"[predict] Loaded {len(df_recent)} rows ({start_date} – {today}).\")\n",
    "print(f\"columns: {', '.join(df_recent.columns)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# ROIPredictor でスコア＆確率を一括生成\n",
    "# ------------------------------\n",
    "predictor = ROIPredictor(model=rank_model, scaler=scaler,\n",
    "                         num_cols=NUM_COLS, device=device, batch_size=512)\n",
    "\n",
    "# (1) スコア（logits）: lane1_score..lane6_score (+ メタ列) を保存\n",
    "pred_scores_df = predictor.predict_scores(df_recent,\n",
    "                                          include_meta=True,\n",
    "                                          save_to=\"artifacts/pred_scores.csv\")\n",
    "display(pred_scores_df.head())\n",
    "\n",
    "\n",
    "# (2) 勝率＆フェアオッズを保存\n",
    "pred_probs_df = predictor.predict_win_probs(scores_df=pred_scores_df,\n",
    "                                            include_meta=True,\n",
    "                                            save_to=\"artifacts/pred_win_probs.csv\")\n",
    "display(pred_probs_df.head())\n",
    "\n",
    "# (3) 馬単/三連単の TOP‑K（PL 方式）を保存\n",
    "exa_df, tri_df = predictor.predict_exotics_topk(scores_df=pred_scores_df,\n",
    "                                                K=10,\n",
    "                                                tau=5.0,\n",
    "                                                include_meta=True,\n",
    "                                                save_exacta=\"artifacts/pred_exacta_topk.csv\",\n",
    "                                                save_trifecta=\"artifacts/pred_trifecta_topk.csv\")\n",
    "display(exa_df.head())\n",
    "display(tri_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce6b7b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# connのクローズ\n",
    "conn.close()\n",
    "print(\"[predict] Prediction completed and saved to artifacts directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c29e0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "#  グループ Ablation: 重要列を 5～6 個まとめてドロップして val_nll を比較\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def permute_importance(model, dataset, device=\"cpu\", cols=None):\n",
    "    \"\"\"\n",
    "    Permutation importance: 各特徴量列をランダムに permute して val_nll の悪化量を調べる\n",
    "    \"\"\"\n",
    "    base_loss = evaluate_model(model, dataset, device)\n",
    "\n",
    "    # ----- 列リストを決める --------------------------------------------------\n",
    "    # cols=None なら「データフレームに存在する “使えそうな” 全列」を対象にする\n",
    "    if cols is None:\n",
    "        # 予測ターゲットやキー列は除外\n",
    "        skip = {\"race_key\", \"race_date\"}\n",
    "        # rank 列（教師信号）や欠損だらけの列も除外\n",
    "        skip |= {c for c in dataset.f.columns if c.endswith(\"_rank\")}\n",
    "        cols = [c for c in dataset.f.columns if c not in skip]\n",
    "\n",
    "    importances: dict[str, float] = {}\n",
    "    df_full = dataset.f\n",
    "\n",
    "    for col in cols:\n",
    "        # --- その列だけランダムに permute ---\n",
    "        shuffled = df_full.copy()\n",
    "        shuffled[col] = np.random.permutation(shuffled[col].values)\n",
    "        tmp_ds = BoatRaceDataset(shuffled)\n",
    "        loss = evaluate_model(model, tmp_ds, device)\n",
    "        importances[col] = loss - base_loss   # 悪化分 (大 → 重要)\n",
    "    return importances\n",
    "\n",
    "def run_ablation_groups(\n",
    "    df_full: pd.DataFrame,\n",
    "    group_size: int = 6,\n",
    "    epochs: int = 5,\n",
    "    seed: int = 42,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    全特徴量をランダムに group_size 個ずつ束ね、\n",
    "    そのグループを丸ごと削除して再学習 → val_nll を返す。\n",
    "\n",
    "    戻り値: list[tuple[list[str], float]]\n",
    "        (ドロップした列リスト, val_nll) を val_nll 昇順で並べたもの\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    essential_cols = set(NUM_COLS)          # ctx 用の連続値\n",
    "    for l in range(1, 7):\n",
    "        essential_cols.update({\n",
    "            f\"lane{l}_exh_time\",\n",
    "            f\"lane{l}_st\",\n",
    "            f\"lane{l}_weight\",\n",
    "            f\"lane{l}_bf_course\",\n",
    "            f\"lane{l}_fs_flag\",\n",
    "            f\"lane{l}_racer_id\",\n",
    "            f\"lane{l}_racer_name\",\n",
    "            f\"lane{l}_racer_age\",\n",
    "            f\"lane{l}_racer_weight\",\n",
    "        })\n",
    "    # --- 対象列を決める（ターゲット & キー列は除外） ---\n",
    "    skip = {\"race_key\", \"race_date\"}\n",
    "    skip |= {c for c in df_full.columns if c.endswith(\"_rank\")}\n",
    "    skip |= essential_cols  \n",
    "    skip |= {c for c in df_full.columns if c.endswith(\"_rank\")}\n",
    "    cols = [c for c in df_full.columns if c not in skip]\n",
    "    random.shuffle(cols)\n",
    "\n",
    "    groups = [cols[i : i + group_size] for i in range(0, len(cols), group_size)]\n",
    "    results = []\n",
    "\n",
    "    latest_date = pd.to_datetime(df_full[\"race_date\"]).dt.date.max()\n",
    "    cutoff = latest_date - dt.timedelta(days=90)\n",
    "\n",
    "    for g in groups:\n",
    "        df_drop = df_full.drop(columns=g)\n",
    "\n",
    "        ds_tr = BoatRaceDataset(df_drop[df_drop[\"race_date\"] < cutoff])\n",
    "        ds_va = BoatRaceDataset(df_drop[df_drop[\"race_date\"] >= cutoff])\n",
    "\n",
    "        ld_tr = DataLoader(ds_tr, batch_size=256, shuffle=True)\n",
    "        ld_va = DataLoader(ds_va, batch_size=512)\n",
    "\n",
    "        model = DualHeadRanker().to(device)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-5)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            model.train()\n",
    "            for ctx, boats, lane_ids, ranks, st_true, st_mask in ld_tr:\n",
    "                ctx, boats = ctx.to(device), boats.to(device)\n",
    "                lane_ids, ranks = lane_ids.to(device), ranks.to(device)\n",
    "                st_true, st_mask = st_true.to(device), st_mask.to(device)\n",
    "                st_pred, scores = model(ctx, boats, lane_ids)\n",
    "                pl_loss = pl_nll_topk(scores, ranks, k=TOPK_K, weights=TOPK_WEIGHTS)\n",
    "                mse_st = ((st_pred - st_true) ** 2 * st_mask.float()).sum() / st_mask.float().sum()\n",
    "                loss = pl_loss + LAMBDA_ST * mse_st\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        val_loss = evaluate_model(model, ds_va, device)\n",
    "        results.append((g, val_loss))\n",
    "\n",
    "    return sorted(results, key=lambda x: x[1])  # 小さい順に重要\n",
    "\n",
    "print(\"▼ Permutation importance (ALL features)\")\n",
    "all_imp = permute_importance(model, ds_val, device)\n",
    "imp_path = \"artifacts/perm_importance_all_base.csv\"\n",
    "pd.Series(all_imp).sort_values(ascending=False).to_csv(imp_path)\n",
    "print(f\"[saved] {imp_path}\")\n",
    "\n",
    "# ② グループ Ablation\n",
    "print(\"▼ Group ablation (drop 6 cols each)\")\n",
    "ab_results = run_ablation_groups(result_df, group_size=6,\n",
    "                                    epochs=5, device=device)\n",
    "abl_path = \"artifacts/ablation_results.csv\"\n",
    "with open(abl_path, \"w\", newline=\"\") as f:\n",
    "    import csv\n",
    "    w = csv.writer(f); w.writerow([\"dropped_cols\", \"val_nll\"])\n",
    "    for cols, v in ab_results:\n",
    "        w.writerow([\"|\".join(cols), f\"{v:.6f}\"])\n",
    "print(f\"[saved] {abl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50387f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# ① SHAP Interaction を計算するユーティリティ\n",
    "# ----------------------------------------------------------------\n",
    "import shap, seaborn as sns, matplotlib.pyplot as plt\n",
    "import numpy as np, torch, os\n",
    "\n",
    "# ① 先頭付近に追加\n",
    "class _ModelForShap(torch.nn.Module):\n",
    "    \"\"\"lane_ids を float → long に戻してから元モデルへ\"\"\"\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "    def forward(self, ctx, boats, lane_ids_f):\n",
    "        return self.base(ctx, boats, lane_ids_f.long())\n",
    "\n",
    "def shap_interaction_heatmap(model, loader_val,\n",
    "                             device=\"cpu\",\n",
    "                             n_samples=128,\n",
    "                             save_prefix=\"artifacts/shap\"):\n",
    "    \"\"\"\n",
    "    ctx(環境6変数) と boat(各艇の3変数: exh_time/st/weight) の\n",
    "    相互作用を DeepExplainer で可視化する。\n",
    "\n",
    "    例：\n",
    "      - フォーム(winrate_30d_l*) × 風速(wind_speed)\n",
    "      - 重量(weight)           × 追い風(wind_sin<0)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(os.path.dirname(save_prefix), exist_ok=True)\n",
    "\n",
    "    # ---- まずバッチを取り出して n_samples だけ切り出す ----\n",
    "    ctx, boats, lane_ids, _, _, _ = next(iter(loader_val))\n",
    "    ctx, boats, lane_ids = ctx[:n_samples].to(device), \\\n",
    "                           boats[:n_samples].to(device), \\\n",
    "                           lane_ids[:n_samples].to(device)\n",
    "\n",
    "    # ---- SHAP DeepExplainer (多入力モデル) -------------------\n",
    "    model_wrap = _ModelForShap(model).to(device)\n",
    "    lane_ids_f = lane_ids.float()\n",
    "    explainer = shap.DeepExplainer(model_wrap, [ctx, boats, lane_ids_f])\n",
    "    # DeepExplainer はバージョンにより shap_interaction_values を\n",
    "    # 実装していないため try / except でフォールバック\n",
    "    try:\n",
    "        shap_int = explainer.shap_interaction_values([ctx, boats, lane_ids_f])\n",
    "        use_exact = True\n",
    "    except AttributeError:\n",
    "        # ---------- 近似的な Interaction 行列を自前計算 ----------\n",
    "        #   |SHAP_i * SHAP_j| の平均 ≈ 交互作用の強さとみなす\n",
    "        try:\n",
    "            shap_vals = explainer.shap_values([ctx, boats, lane_ids_f], check_additivity=False)\n",
    "        except AssertionError:\n",
    "            # DeepExplainer が sum-consistency でコケたときは GradientExplainer に切替\n",
    "            explainer = shap.GradientExplainer(model_wrap, [ctx, boats, lane_ids_f])\n",
    "            shap_vals = explainer.shap_values([ctx, boats, lane_ids_f], check_additivity=False)\n",
    "        ctx_sv   = np.asarray(shap_vals[0])        # shape (B,C)\n",
    "        boat_sv  = np.asarray(shap_vals[1])        # shape (B,6,F)\n",
    "\n",
    "        # 1) 環境 × 環境\n",
    "        ctx_int_mat = np.mean(np.abs(ctx_sv[:, :, None] * ctx_sv[:, None, :]), axis=0)  # (C,C)\n",
    "\n",
    "        # 2) Boat × Boat (lane 平均)\n",
    "        #    boat_sv: (B,6,F) → (B,6,F,1) * (B,6,1,F) → (B,6,F,F)\n",
    "        boat_pair = np.abs(boat_sv[:, :, :, None] * boat_sv[:, :, None, :])  # (B,6,F,F)\n",
    "        boat_int_mat = boat_pair.mean((0,1))                                 # lane & batch 平均 → (F,F)\n",
    "\n",
    "        # 3) 環境 × Boat\n",
    "        cross_raw = np.abs(ctx_sv[:, :, None, None] * boat_sv[:, None, :, :])  # (B,C,6,F)\n",
    "        cross_int = cross_raw.mean((0,2))                                       # (C,F)\n",
    "        use_exact = False\n",
    "    else:\n",
    "        # ---------- 正確な Interaction (DeepExplainer 対応版) ----------\n",
    "        # shap_int は list: [ctx_int, boat_int, lane_int]\n",
    "        #   ctx_int  : (B, C, C)\n",
    "        #   boat_int : (B, 6, F, 6, F)\n",
    "        ctx_int_mat  = np.abs(shap_int[0]).mean(0)                     # (C,C)\n",
    "        boat_int_raw = np.abs(shap_int[1])                             # (B,6,F,6,F)\n",
    "        boat_int_mat = boat_int_raw.mean((0,1,3))                      # (F,F)\n",
    "\n",
    "        cross_int    = boat_int_raw.mean((0,3))                        # (C,F)\n",
    "    msg = \"exact\" if use_exact else \"approx\"\n",
    "    print(f\"[info] SHAP interaction mode: {msg}\")\n",
    "\n",
    "        # --- ensure ctx_int_mat is 2‑D (Seaborn heatmap requirement) ---\n",
    "    while ctx_int_mat.ndim > 2:\n",
    "        ctx_int_mat = ctx_int_mat.mean(0)\n",
    "\n",
    "    # ---- ヒートマップ表示 ------------------------------------\n",
    "    # (A) 環境どうし\n",
    "    ctx_feat_names = [\"air_temp\",\"wind_speed\",\"wave_height\",\n",
    "                      \"water_temp\",\"wind_sin\",\"wind_cos\"]\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(ctx_int_mat, annot=False, square=True,\n",
    "                xticklabels=ctx_feat_names, yticklabels=ctx_feat_names,\n",
    "                cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"SHAP Interaction – Environment × Environment\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_ctx_ctx.png\", dpi=180)\n",
    "    plt.show()\n",
    "\n",
    "    # (B) Boat どうし\n",
    "        # --- ensure boat_int_mat is 2‑D ---\n",
    "    while boat_int_mat.ndim > 2:\n",
    "        # 平均を最後の軸に取って次元を潰す\n",
    "        boat_int_mat = boat_int_mat.mean(-1)\n",
    "    # feature 数とラベル長が合わなければ自動補完\n",
    "    \n",
    "    boat_feat_names = [\"exh_time\",\"st\",\"weight\"]         # ← BoatRaceDataset が渡す順\n",
    "    F = boat_int_mat.shape[0]\n",
    "    if len(boat_feat_names) != F:\n",
    "        boat_feat_names = [f\"feat{i}\" for i in range(F)]\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.heatmap(boat_int_mat, annot=False, square=True,\n",
    "                xticklabels=boat_feat_names, yticklabels=boat_feat_names,\n",
    "                cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"SHAP Interaction – Boat × Boat (lane平均)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_boat_boat.png\", dpi=180)\n",
    "    plt.show()\n",
    "\n",
    "    # (C) 環境 × Boat\n",
    "    while cross_int.ndim > 2:\n",
    "        # 平均を第1軸に取りつつ次元を潰す\n",
    "        cross_int = cross_int.mean(1)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cross_int, annot=False,\n",
    "                yticklabels=ctx_feat_names,\n",
    "                xticklabels=boat_feat_names,\n",
    "                cmap=\"coolwarm\", center=0)\n",
    "    plt.title(\"SHAP Interaction – Environment × Boat\")\n",
    "    plt.xlabel(\"Boat feature\"), plt.ylabel(\"Env feature\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_prefix}_ctx_boat.png\", dpi=180)\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 具体的なペアの数値を抜き出す例 ----------------------\n",
    "    # w_speed_idx   = ctx_feat_names.index(\"wind_speed\")\n",
    "    # headwind_idx  = ctx_feat_names.index(\"wind_sin\")   # sin<0 が追い風\n",
    "    # weight_idx    = boat_feat_names.index(\"weight\")\n",
    "\n",
    "    # form_idxs = [i for i,n in enumerate(boat_feat_names) if \"winrate\" in n]  # 例\n",
    "    # form_wspeed = cross_int[w_speed_idx, weight_idx]\n",
    "    # weight_head = cross_int[headwind_idx, weight_idx]\n",
    "    # print(f\"[form × wind_speed] interaction ≈ {form_wspeed:.4f}\")\n",
    "    # print(f\"[weight × headwind] interaction ≈ {weight_head:.4f}\")\n",
    "\n",
    "# ─── 呼び出し ───\n",
    "shap_interaction_heatmap(model, loader_val, device=device, n_samples=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f3edc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================\n",
    "# ④ ── 「勾配が流れているか」を瞬時に確認する Snippet\n",
    "#       （エポック終了後 1 回だけ走らせれば十分）\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ============================================================\n",
    " \n",
    " # ============================================================\n",
    " # ⑤ ── 超小規模データで「過学習できるか」テスト関数\n",
    " #       必要時に呼び出して 0.1 以下まで loss が落ちるか確認\n",
    " # -----------------------------------------------------------\n",
    "\n",
    "\n",
    "# ---- tiny データで特徴量の分散を確認 -----------------------\n",
    "tiny_df = result_df.sample(10, random_state=1).reset_index(drop=True)\n",
    "num_cols = tiny_df.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# (1) 行間（=レース間）での分散\n",
    "print(\"► 行間 variance (should be >0):\")\n",
    "print(tiny_df[num_cols].var().nsmallest(10))\n",
    "\n",
    "# (2) 同一レース内（= 6 艇間）での分散\n",
    "def per_race_var(col):\n",
    "    return tiny_df.groupby(\"race_key\")[col].var().mean()\n",
    "\n",
    "per_race = {c: per_race_var(c) for c in num_cols}\n",
    "print(\"\\n► 6 艇間 variance:\")\n",
    "print(sorted(per_race.items(), key=lambda x: x[1])[:10])\n",
    "\n",
    "# ---- 呼び方例 ----\n",
    "overfit_tiny(result_df, device)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7af24a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.save({\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"scaler\": scaler_filename,\n",
    "    \"venue2id\": venue2id,\n",
    "    # \"race_type2id\": race_type2id\n",
    "}, \"cplnet_checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "boat_racing-zew2npIb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
